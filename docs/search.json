[
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "The Practice R Tutorials",
    "section": "Preface",
    "text": "Preface\nThis website gives access to all tutorials of Practice R (Treischl 2023). Practice R is a text book for the social sciences which provides several tutorials supporting students to learn R. Feel free to inspect the tutorials even if you are not familiar with the book, but keep in mind the tutorials are supposed to complement the Practice R book."
  },
  {
    "objectID": "index.html#the-book",
    "href": "index.html#the-book",
    "title": "The Practice R Tutorials",
    "section": "The book",
    "text": "The book\nMany students learn to analyze data using commercial packages, even though there is an open-source software with cutting-edge possibilities: R, a programming language with countless cool features for applied empirical research.\nPractice R introduces R to social science students, inspiring them to consider R as an excellent choice. In a non-technical pragmatic way, this book covers all typical steps of applied empirical research.\nLearn how to prepare, analyze, and visualize data in R. Discover how to collect data, generate reports, or automate error-prone tasks.\nThe book is accompanied by an R package. This provides further learning materials that include interactive tutorials, challenging you with typical problems of applied research. This way, you can immediately practice the knowledge you have learned. The package also includes the source code of each chapter and templates that help to create reports.\nPractice R has social science students in mind, nonetheless a broader audience may use Practice R to become a proficient R user.\n\nIntroduces R in a non-technical fashion\nCovers typical steps of applied empirical research\nComplemented by interactive tutorials\nWith access to all materials via the Practice R Package\n\n\n\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg."
  },
  {
    "objectID": "chapter_02.html#typical-error-messages",
    "href": "chapter_02.html#typical-error-messages",
    "title": "1  Base R",
    "section": "1.1 Typical error messages",
    "text": "1.1 Typical error messages\nWhat kind of errors do we need to talk about? Sometimes we introduce errors when we are not cautious enough about the code. Spelling mistakes (e.g., typos, missing and wrong characters, etc.) are easy to fix yet hard to find. For example, I tried to use the assignment operator, but something went wrong. Do you know what might be the problem?\n\n#Assigning the values the wrong way\na -&lt; 5\nb -&lt; 3\n\na + b\n\n#&gt; Error: &lt;text&gt;:2:4: unexpected '&lt;'\n#&gt; 1: #Assigning the values the wrong way\n#&gt; 2: a -&lt;\n#&gt;       ^\n\n\n\n# Keep the short cut for the assignment operator in mind:\n#&lt;Alt/Option&gt; + &lt;-&gt;\n\n# Solution:\na &lt;- 5\nb &lt;- 3\n\na + b\n\n#&gt; [1] 8\n\n\nFinding spelling mistakes in your own code can be hard. There are certainly several reasons, but our human nature to complete text certainly is part of it. This ability gives us the possibility to read fast, but it makes it difficult to see our own mistakes. Don’t get frustrated, it happens even if you have a lot of experience working with R. Thus, check if there are no simple orthographically mistakes - such as typos, missing (extra) parentheses, and commas - which prevents the code from running.\nI highlighted in Chapter 2 that RStudio inserts opening and closing parentheses, which reduces the chance that missing (or wrong) characters create an error, but there is no guarantee that we insert or delete one by chance. Suppose you try to estimate a mean in combination with the round() function. I put a parenthesis at a wrong place, which is why R throws an error. Can you see which parenthesis is causing the problem?\n\n#Check parenthesis\nround(mean(c(1, 4, 6))), digits = 2)\n\n#&gt; Error: &lt;text&gt;:2:24: unexpected ','\n#&gt; 1: #Check parenthesis\n#&gt; 2: round(mean(c(1, 4, 6))),\n#&gt;                           ^\n\n\n\n# Solution:\nround(mean(c(1, 4, 6)), digits = 2)\n\n#&gt; [1] 3.67\n\n\nThis error is hard to spot, but it illustrates that we need to be careful not to introduce mistakes. Moreover, RStudio gives parentheses that belong together the same color which help us to keep overview. Go to the RStudio menu (via the &lt;Code&gt; tab) and select rainbow parentheses if they are not displayed in color in the Code pane.\nUnfortunately, RStudio cannot help us all the time because some R errors messages (and warnings) are cryptic. There are even typical errors messages that are quite obscure for beginners. For example, R tells me all the time that it can’t find an object, functions, and data. There are several explanations why R throws such an error. If R cannot find an object, check if the object is listed in the environment. If so, you know for sure that the object exists and that other reasons cause the error. R cannot find an object even in the case of a simple typo.\n\n# R cannot find an object due to typos\nmean_a &lt;- mean(1, 2, 3)\nmaen_a\n\n#&gt; Error in eval(expr, envir, enclos): object 'maen_a' not found\n\n\n\n# Solution:\nmean_a &lt;- mean(1, 2, 3)\nmean_a\n\n#&gt; [1] 1\n\n\nR tells us that a function (an object) cannot be found if different notations are used. Keep in mind that R is case-sensitive (r vs. R) and cannot apply a function (or find an object) that does not exist, as the next console illustrates. Of course, the same applies if you forgot to execute the function before using it or if the function itself includes an error and cannot be executed. In all these examples R cannot find the function (or object).\n\n# R is case-sensitive\nreturn_fun &lt;- function(x) {\n  return(x)\n}\n\nReturn_fun(c(1, 2, 3))\n\n#&gt; Error in Return_fun(c(1, 2, 3)): could not find function \"Return_fun\"\n\n\n\n# Solution:\nreturn_fun(c(1, 2, 3))\n\n#&gt; [1] 1 2 3\n\n\nWhat is the typical reason why a function from an R package cannot be found? I started to introduce the dplyr package in Chapter 2 (Wickham et al. 2022). Suppose we want to use the select function from the package. To use anything from an R package, we need to load the package with the library() function each time we start (over). Otherwise, R cannot find the function.\n\n# Load the package to use a function from a package\nlibrary(palmerpenguins)\nselect(penguins, species)\n\n#&gt; Error in select(penguins, species): could not find function \"select\"\n\n\n\n# Solution:\ndplyr::select(penguins, species)\n\n#&gt; # A tibble: 344 × 1\n#&gt;    species\n#&gt;    &lt;fct&gt;  \n#&gt;  1 Adelie \n#&gt;  2 Adelie \n#&gt;  3 Adelie \n#&gt;  4 Adelie \n#&gt;  5 Adelie \n#&gt;  6 Adelie \n#&gt;  7 Adelie \n#&gt;  8 Adelie \n#&gt;  9 Adelie \n#&gt; 10 Adelie \n#&gt; # ℹ 334 more rows\n\n\nThe same applies to objects from a package (e.g., data). The .packages() function returns all loaded (attached) packages, but there is no need to keep that in mind. Go to the packages pane and check if a package is installed and loaded. R tells us only that the function cannot be found if we forget to load it first.\n\n# Inspect the loaded packages via the Packages pane\nloaded_packages &lt;- .packages()\nloaded_packages\n\n#&gt; [1] \"palmerpenguins\" \"stats\"          \"graphics\"       \"grDevices\"     \n#&gt; [5] \"utils\"          \"datasets\"       \"methods\"        \"base\"\n\n\nUltimately, suppose we try to import data. Never mind about the code, we focus on this step in Chapter 5 in detail, but R tells us that it cannot open the connection if the file cannot be found in the current working directory.\n\n# Load my mydata\nread.csv(\"mydata.csv\")\n\n#&gt; Warning in file(file, \"rt\"): cannot open file 'mydata.csv': No such file or\n#&gt; directory\n\n\n#&gt; Error in file(file, \"rt\"): cannot open the connection\n\n\nR tells that data, or other files cannot be found because we provided the wrong path to the file. We will learn how to import data later, but keep in mind that R cannot open a file if we search in the wrong place. In Chapter 2, I outlined many possibilities to change the work directory for which RStudio supplies convenient ways. In addition, the getwd() function returns the current work directory in case of any doubts.\n\n# Do we search for files in the right place\ngetwd()\n#&gt; [1] \"C:/Users/Edgar/R/Practice_R/Tutorial/02\"\n\nLoading the right packages and searching in the right place does not imply that we cannot inadvertently introduce mistakes. Suppose you want to apply the filter function from the dplyr package. You copy and adjust the code from an old script, but R returns an error. Can you see where I made the mistake? I tried to create a subset with Adelie penguins only, but dplyr seems to know what the problem might be.\n\n# Mistakes happen all the time ...\nlibrary(dplyr)\nfilter(penguins, species = \"Adelie\")\n\n#&gt; Error in `filter()`:\n#&gt; ! We detected a named input.\n#&gt; ℹ This usually means that you've used `=` instead of `==`.\n#&gt; ℹ Did you mean `species == \"Adelie\"`?\n\n\n\n# Solution:\nlibrary(dplyr)\nfilter(penguins, species == \"Adelie\")\n\n#&gt; # A tibble: 152 × 8\n#&gt;    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#&gt;    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n#&gt;  1 Adelie  Torgersen           39.1          18.7               181        3750\n#&gt;  2 Adelie  Torgersen           39.5          17.4               186        3800\n#&gt;  3 Adelie  Torgersen           40.3          18                 195        3250\n#&gt;  4 Adelie  Torgersen           NA            NA                  NA          NA\n#&gt;  5 Adelie  Torgersen           36.7          19.3               193        3450\n#&gt;  6 Adelie  Torgersen           39.3          20.6               190        3650\n#&gt;  7 Adelie  Torgersen           38.9          17.8               181        3625\n#&gt;  8 Adelie  Torgersen           39.2          19.6               195        4675\n#&gt;  9 Adelie  Torgersen           34.1          18.1               193        3475\n#&gt; 10 Adelie  Torgersen           42            20.2               190        4250\n#&gt; # ℹ 142 more rows\n#&gt; # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nTypos, missing functions (objects), and confusion about operators are typical mistakes and some packages return suggestions to fix the problem. Unfortunately, R can also return cryptic error messages, which are often harder to understand."
  },
  {
    "objectID": "chapter_02.html#cryptic-errors",
    "href": "chapter_02.html#cryptic-errors",
    "title": "1  Base R",
    "section": "1.2 Cryptic errors",
    "text": "1.2 Cryptic errors\nNot all error R messages and warnings are cryptic. Suppose you wanted to estimate a mean of an income variable. The variable is not measured numerically which implies that the mean cannot be estimated. Consequently, R warns us about wrong and inconsistent data types.\n\n# Warning: argument is not numeric or logical\nincome &lt;- c(\"More than 10000\", \"0 - 999\", \"2000 - 2999\")\nmean(income)\n\n#&gt; Warning in mean.default(income): argument is not numeric or logical: returning\n#&gt; NA\n\n\nUnfortunately, some errors and warnings seem more like an enigma than useful feedback. Imagine, R tells you that a non-numeric argument has been applied to a binary operator. The next console reproduces the error with two example vectors. The last value of the vector y is a character (e.g., a missing value indicator: NA) and for obvious reasons we cannot multiply x with y as long as we do clean the latter.\n\n# Cryptic error: A non-numeric argument to binary operator\nx &lt;- c(3, 5, 3)\ny &lt;- c(1, 4, \"NA\")\n\nresult &lt;- x * y\n\n#&gt; Error in x * y: non-numeric argument to binary operator\n\nresult\n\n#&gt; Error in eval(expr, envir, enclos): object 'result' not found\n\n\nWe will learn how to fix such problem in a systematic manner later, for now just keep in mind that such an error message might be due to messy, not yet prepared data. Or suppose you tried to estimate the sum but R tells you that the code includes an unexpected numeric constant. Any idea what that means and how to fix the example code of the next console?\n\n#Cryptic error: Unexpected numeric constant\nsum(c(3, 2 1))\n\n#&gt; Error: &lt;text&gt;:2:12: unexpected numeric constant\n#&gt; 1: #Cryptic error: Unexpected numeric constant\n#&gt; 2: sum(c(3, 2 1\n#&gt;               ^\n\n\n\n# Solution:\nsum(c(3, 2, 1))\n\n#&gt; [1] 6\n\n\nR finds an unexpected numeric constant (here 1) because I forgot the last comma inside the c() function. The same applies to strings and characters. R tells us that there is an unexpected string constant. Can you see where?\n\n#Cryptic error: Unexpected string constant\nnames &lt;- c(\"Tom\", \"Diana\"___\"Pete\")\nnames\n\n#&gt; Error: &lt;text&gt;:2:26: unexpected input\n#&gt; 1: #Cryptic error: Unexpected string constant\n#&gt; 2: names &lt;- c(\"Tom\", \"Diana\"_\n#&gt;                             ^\n\n\n\n# Solution:\nnames &lt;- c(\"Tom\", \"Diana\", \"Pete\")\nnames\n\n#&gt; [1] \"Tom\"   \"Diana\" \"Pete\"\n\n\nOr consider unexpected symbols. Can you find the problem of the next console. I used to round function but something went wrong with the digits option.\n\n#Cryptic error: Unexpected symbol\nx &lt;- mean(c(1:3))\nround(x digits = 2)\n\n#&gt; Error: &lt;text&gt;:3:9: unexpected symbol\n#&gt; 2: x &lt;- mean(c(1:3))\n#&gt; 3: round(x digits\n#&gt;            ^\n\n\n\n# Solution:\nx &lt;- mean(c(1:3))\nround(x, digits = 2)\n\n#&gt; [1] 2\n\n\nThus, we introduce a mistake with a function argument because the comma is missing. A similar mistake happens if we forget to provide a necessary argument or provide a wrong one. For example, there is no numbers option of the round function as the next console (and the help files ?round) outline.\n\n# Cryptic error: Unused argument\nx &lt;- mean(c(1:3))\nround(x, numbers = 2)\n\n#&gt; Error in round(x, numbers = 2): unused argument (numbers = 2)\n\n\n\n# Solution:\nx &lt;- mean(c(1:3))\nround(x, digits = 2)\n\n#&gt; [1] 2\n\n\nTry to be patient and be kind to yourself should you run into such an error. You will become better to solve errors, but they will happen all the time. Let me give you one more for the road. Consider the error message: object of type ‘closure’ is not subsettable. R returns this error message if we try to slice a variable that does not exist or if we try to slice a function instead of providing a column vector. Can you fix the next console and provide a column vectors instead of slicing the mean() function?\n\n# Cryptic error: Object of type 'closure' is not subsettable\nmean[1:5]\n\n#&gt; Error in mean[1:5]: object of type 'closure' is not subsettable\n\n\n\n# Solution:\nmean(1:5)\n\n#&gt; [1] 3"
  },
  {
    "objectID": "chapter_02.html#further-sources-of-errors",
    "href": "chapter_02.html#further-sources-of-errors",
    "title": "1  Base R",
    "section": "1.3 Further sources of errors",
    "text": "1.3 Further sources of errors\nThere are further errors and mistakes and this tutorial cannot capture them all. As a minimum, I try to give you a heads-up that it takes time and experience to overcome such problems. For example, consider one more time the small data that we used to slice data in Practice R.\n\n# Save data as df\ndf &lt;- tibble::tribble(\n  ~names, ~year, ~sex,\n  \"Bruno\", 1985, \"male\",\n  \"Justin\", 1994, \"male\",\n  \"Miley\", 1992, \"female\",\n  \"Ariana\", 1993, \"female\"\n)\n\nDo you still remember how to slice the data? Give it a try with the following examples:\n\n# Slice the first column (variable)\ndf[1]\n\n#&gt; # A tibble: 4 × 1\n#&gt;   names \n#&gt;   &lt;chr&gt; \n#&gt; 1 Bruno \n#&gt; 2 Justin\n#&gt; 3 Miley \n#&gt; 4 Ariana\n\n\n\n# First row\ndf[1, ]\n\n#&gt; # A tibble: 1 × 3\n#&gt;   names  year sex  \n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 Bruno  1985 male\n\n\nSuppose that you have not worked with R for a few weeks, would you still be able to remember how slicing works? We all face the same problems when we start to learn something new: you need several attempts before you understand how to get the desired information. Later, after slicing data many times, you will no longer think about how it works. Thus, be patient and kind to yourself, because some concepts need time and experience to internalize them.\nMoreover, there are often several approaches to reach the same goal and - depending on your preferred style - some are harder or easier to apply. Say you need the names of the stars as a column vector. Can you slice the data or use the $ operator to get the names variable from the data frame?\n\n# Slice or use the $ operator\nnames &lt;- df$names\nnames &lt;- df[1]\nnames\n\n#&gt; # A tibble: 4 × 1\n#&gt;   names \n#&gt;   &lt;chr&gt; \n#&gt; 1 Bruno \n#&gt; 2 Justin\n#&gt; 3 Miley \n#&gt; 4 Ariana\n\n\nUnfortunately, some mistakes are logical in nature and pure practice cannot help us to overcome such problems. Consider the next console. I created a slice function (slice_function) which is supposed to return an element of a vector x, but so far it only returns non-sense. Why does it not return the second element of the input data?\n\n# A pretty messed up slice_function\ndata &lt;- c(3, 9, 1, 5, 8, \"999\", 1)\n\nslice_function &lt;- function(data, x) {\n  data[x]\n}\n\nslice_function(2)\n\n#&gt; [1] 2\n\n\n\n# Solution:\ndata &lt;- c(3, 9, 1, 5, 8, 1)\n\nslice_function &lt;- function(data, x) {\n  data[x]\n}\n\nslice_function(data, x = 2)\n\n#&gt; [1] 9\n\n\nSoon, your code will encompass several steps, try to break it into its separate elements and then examine each step carefully. For example, inspect the vector x to see if error was introduced in the first step. Use the class() function to examine if the input of a variable is as expected (e.g. numerical). If we are sure about the input, we would go on to the next step and so on. Certainly, the last example is not complicated but the complexity of code (and the tasks) will increase from the chapter to chapter. By breaking down all steps into elements, you may realize where the error occurs and how you can fix it."
  },
  {
    "objectID": "chapter_02.html#summary",
    "href": "chapter_02.html#summary",
    "title": "1  Base R",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nAll tutorials of Practice R will end with a short code summary of the corresponding book chapter. The summary only contains the function name from the R help file and code example of the most important functions and packages. In connection with Chapter 2, keep the following functions in mind:\n\nInstall packages from repositories or local files (install.packages)\nLoading/attaching and listing of packages(library)\nInspect the help file (?function)\nCombine Values into a vector or list (c)\nCompare objects (&lt;=, &gt;=, ==, !=)\nReplicate elements of vectors and lists (rep)\nSequence generation (seq)\nSum of vector elements (sum)\nLength of an object (length)\nObject classes (class)\nData frames (data.frame)\nBuild a data frame (tibble::tibble, Müller and Wickham 2022)\nRow-wise tibble creation (tibble::tribble)\nThe number of rows/columns of an array (nrow/ncol)\n\nBase R and many R packages have cheat sheets that summarize the most important features. You can inspect them directly from RStudio (via the &lt;help&gt; tab) and I included the link to the base R cheat sheet in the PracticeR package.\n\n# Cheat sheets summarize the most important features\n# The base R cheat sheet\nPracticeR::show_link(\"base_r\")\n\n\n\n\n\nMüller, Kirill, and Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "chapter_03.html#categorical-variables",
    "href": "chapter_03.html#categorical-variables",
    "title": "2  Data Exploration",
    "section": "2.1 Categorical variables",
    "text": "2.1 Categorical variables\nWe started to explore categorical variables in Chapter 3 and I outlined a few basics about factor variables. Suppose we want to explore the factor variable island, which indicates where the penguins live. How can you examine unique group levels?\n\n# Inspect the levels() of the penguin's home island\nlevels(df$island)\n\n#&gt; [1] \"Biscoe\"    \"Dream\"     \"Torgersen\"\n\n\nWe will deepen our knowledge about factor variables in Chapter 5, but keep in mind that we can (re-) create and adjust factor() variables. For example, suppose the data looks like a messy character vector for penguin’s sex that I have created in the next console. In such a case it is good to remember that we can give the variable proper text labels (e.g., female for f) and examine the results.\n\n# Example of a messy factor variable\nsex &lt;- c(\"m\", \"f\", \"f\")\n\n# Give clearer labels\nsex &lt;- factor(sex,\n  levels = c(\"f\", \"m\"),\n  labels = c(\"female\", \"male\"),\n)\nhead(sex)\n\n#&gt; [1] male   female female\n#&gt; Levels: female male\n\n\nTables help us to explore data and we used the summarytools package to make frequency and cross tables (Comtois 2022). Keep in mind that we will learn how to create text documents with tables and graphs in Chapter 8. For the moment it is enough to remember that we can create different sort of tables with the summarytools package. For example, create a frequency (freq) table to find out on which island most of the penguins live.\n\n# Create a frequency table\nfreq(df$island)\n\n#&gt; Frequencies  \n#&gt; df$island  \n#&gt; Type: Factor  \n#&gt; \n#&gt;                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n#&gt; --------------- ------ --------- -------------- --------- --------------\n#&gt;          Biscoe    168     48.84          48.84     48.84          48.84\n#&gt;           Dream    124     36.05          84.88     36.05          84.88\n#&gt;       Torgersen     52     15.12         100.00     15.12         100.00\n#&gt;            &lt;NA&gt;      0                               0.00         100.00\n#&gt;           Total    344    100.00         100.00    100.00         100.00\n\n\nAs outlined in the book, we can use the table() function to count categorical variables and plot the result as a bar graph. I introduced the latter approach because it is very easy to apply, but our code becomes clearer if we make the necessary steps visible. First, we need to count the levels before we can plot the results. The count() function from the dplyr package does this job (Wickham et al. 2022). It needs only the data frame and the factor variable.\n\n# Count islands with dplyr\ncount_island &lt;- dplyr::count(df, island)\ncount_island\n\n#&gt; # A tibble: 3 × 2\n#&gt;   island        n\n#&gt;   &lt;fct&gt;     &lt;int&gt;\n#&gt; 1 Biscoe      168\n#&gt; 2 Dream       124\n#&gt; 3 Torgersen    52\n\n\nNext, use the assigned results (count_island) and insert the variables into the barplot() function (with the formula y ~ x).\n\n# Create a barplot\nbarplot(n ~ island, data = count_island)\n\n\n\n\n\nIn a similar vein, I introduced functions from the DataExplorer package that help us to get a quick overview (Cui 2020). For example, use the plot_bar() function to depict several or all discrete variables of a data frame.\n\n# Inspect all or several plots at once\nDataExplorer::plot_bar(df[1:2])"
  },
  {
    "objectID": "chapter_03.html#continuous-variables",
    "href": "chapter_03.html#continuous-variables",
    "title": "2  Data Exploration",
    "section": "2.2 Continuous variables",
    "text": "2.2 Continuous variables\nTo explore continuous variables, estimate the summary statistics with the summary() function. Pick one variable such as penguin’s body mass in gram (body_mass_g) or use the entire data frame.\n\n# Get a summary\nsummary(df[1:4])\n\n#&gt;       species          island    bill_length_mm  bill_depth_mm  \n#&gt;  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n#&gt;  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n#&gt;  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n#&gt;                                  Mean   :43.92   Mean   :17.15  \n#&gt;                                  3rd Qu.:48.50   3rd Qu.:18.70  \n#&gt;                                  Max.   :59.60   Max.   :21.50  \n#&gt;                                  NA's   :2       NA's   :2\n\n\nThe classic approach to visualize the distribution of a continuous variable is a histogram. Use the hist() function to display the distribution of the penguins body mass.\n\n# Create a histogram\nhist(df$body_mass_g)\n\n\n\n\nKeep in mind that we only explored the data for the first time. We did not clean the data nor did we prepare the variables. We have to be explicit about missing values when we want to apply functions such as the mean. The function returns NA, but only because of a missing values problem. Can you remember how to fix this problem and estimate, for example, the mean?\n\n# Calculate the mean, but what about missing values (na.rm)?\nmean(df$body_mass_g, na.rm = TRUE)\n\n#&gt; [1] 4201.754\n\n\nI picked data that was more or less prepared to be explored, because data preparation needs more time and effort especially in the beginning. For this reason we will learn how to manipulate data in Chapter 4; and Chapter 5 tries to prepare you for own journey. For example, we use packages such as visdat and naniar to identify missing values, as the next console illustrates with two examples (Tierney et al. 2021). The vis_dat() function from the corresponding packages shows us which type of data we have with missing values in gray; while vis_miss() visualizes missing values in general terms. Keep in mind that Chapter 3 did not introduce data preparation steps which are often necessary to explore data and effects between variables.\n\nlibrary(visdat)\n\n# Left plot: vis_dat()\nvis_dat(df)\n\n# Right plot: vis_miss()\nvis_miss(df)"
  },
  {
    "objectID": "chapter_03.html#explore-effects",
    "href": "chapter_03.html#explore-effects",
    "title": "2  Data Exploration",
    "section": "2.3 Explore effects",
    "text": "2.3 Explore effects\nLet’s start with an effect between two categorical variables. There are different packages that provides functions to create (cross) tables, but we used the summarytools package. It even provides a simulated data set which we will use the repeat the steps to create a cross table. The package comes with the tobacco data, which illustrates that smoking is harmful. As the next console shows, it indicates if a person is a smoker and if the person is diseased.\n\nhead(tobacco)[1:8]\n\n#&gt;   gender age age.gr      BMI smoker cigs.per.day diseased      disease\n#&gt; 1      M  75   71 + 29.50225     No            0       No         &lt;NA&gt;\n#&gt; 2      F  35  35-50 26.14989     No            0      Yes Neurological\n#&gt; 3      F  70  51-70 27.53183     No            0       No         &lt;NA&gt;\n#&gt; 4      F  40  35-50 24.05832     No            0       No         &lt;NA&gt;\n#&gt; 5      F  75   71 + 22.77486     No            0      Yes      Hearing\n#&gt; 6      M  38  35-50 21.46412     No            0       No         &lt;NA&gt;\n\n\nUse the ctable function from the summarytools package to make a cross table for these variables. See also what happens if you adjust the prop option. Insert c or t. Furthermore, explore what happens if you set the chisq, OR, or RR option to TRUE.\n\n# Create a cross table with summarytools\nsummarytools::ctable(\n  x = tobacco$smoker,\n  y = tobacco$diseased,\n  prop = \"r\",\n  chisq = TRUE,\n  OR = TRUE\n)\n\n#&gt; Cross-Tabulation, Row Proportions  \n#&gt; smoker * diseased  \n#&gt; Data Frame: tobacco  \n#&gt; \n#&gt; \n#&gt; -------- ---------- ------------- ------------- ---------------\n#&gt;            diseased           Yes            No           Total\n#&gt;   smoker                                                       \n#&gt;      Yes              125 (41.9%)   173 (58.1%)    298 (100.0%)\n#&gt;       No               99 (14.1%)   603 (85.9%)    702 (100.0%)\n#&gt;    Total              224 (22.4%)   776 (77.6%)   1000 (100.0%)\n#&gt; -------- ---------- ------------- ------------- ---------------\n#&gt; \n#&gt; ----------------------------\n#&gt;  Chi.squared   df   p.value \n#&gt; ------------- ---- ---------\n#&gt;    91.7088     1       0    \n#&gt; ----------------------------\n#&gt; \n#&gt; ----------------------------------\n#&gt;  Odds Ratio   Lo - 95%   Hi - 95% \n#&gt; ------------ ---------- ----------\n#&gt;     4.40        3.22       6.02   \n#&gt; ----------------------------------\n\n\nThe prop option lets you determine the proportions: rows (r), columns (c), total (t), or none (n). Furthermore, the function even adds the chi-square statistic (chisq); the odds ratio (OR) or the relative risk (RR) if we set them to TRUE. Never mind if you are not familiar with the latter, the discussed options only illustrated how the summarytools package helps us to explore data and effects.\nIn the social sciences we are often interested in comparing numerical outcomes between categorical variables (groups). For example, one of the penguin’s species has a higher body mass and we can examine which penguins species differ in terms of their body mass (body_mass_g). With base R, the aggregate() function lets us split the data and we are able to estimate the mean for each species.\n\n# Aggregate splits the data into subsets and computes summary statistics\naggregate(df$body_mass_g, list(df$species), FUN = mean, na.rm = TRUE)\n\n#&gt;     Group.1        x\n#&gt; 1    Adelie 3700.662\n#&gt; 2 Chinstrap 3733.088\n#&gt; 3    Gentoo 5076.016\n\n\nTo calculate a group-mean looks quite complicated and I did not introduce the latter since we will systematically work on our skills to manipulate data in the next Chapter. Instead, we used a box plot to explore a continuous outcome between groups. As outlined in the book, box plots can be very helpful to compare groups even though they have graphical limitations since they do not display the data. Keep the boxplot() function in mind and practice one more time how it works. Inspect how penguin’s body mass differs between the species.\n\n# Inspect group differences with a box plot\nboxplot(body_mass_g ~ species, data = df)\n\n\n\n\nIf we examine an effect between two continuous outcomes, we have to keep in mind that the plot function returns a scatter plot and we may insert a regression line with the abline and the lm function. Do you still know how it works? Create a scatter plot to examine the association between the body mass (body_mass_g) and the flipper length (flipper_length_mm) of the penguins.\n\n# Create a scatter plot\nplot(y = df$body_mass_g, x = df$flipper_length_mm)\n\n# And a red regression line\nabline(lm(body_mass_g ~ flipper_length_mm, data = df),\n  col = \"red\"\n)\n\n\n\n\nFurthermore, we learned how to calculate the correlation coefficient. The code of the next console does not work if I apply the cor() with the penguins data. Do you have any idea how to fix the problem?\n\n# Calculate the correlation between x and y\ncor_penguins &lt;- cor(df$body_mass_g, df$flipper_length_mm,\n  use = \"complete\"\n)\ncor_penguins\n\n#&gt; [1] 0.8712018\n\n\nBy the way, the cor() also returns Kendall’s or Spearman’s if you adjust the method option:\n\n# estimate a rank-based measure of association\ncor(x,\n  y = NULL, use = \"complete\",\n  method = c(\"pearson\", \"kendall\", \"spearman\")\n)\n\n\nFinally, the effectsize package helped us with the interpretation of Pearson’s r (and other stats, see Chapter 6). I copied the code from the book; can you adjust it to interpret the effect of the examined variables with the effectsize package (Ben-Shachar et al. 2022)?\n\n\n#&gt; [1] 0.8712018\n\n\n\n# Use effectsize to interpret R\neffectsize::interpret_r(cor_penguins, rules = \"cohen1988\")\n\n#&gt; [1] \"large\"\n#&gt; (Rules: cohen1988)\n\n\n\nThere are more R packages to explore data than I could possibly outline. For example, consider the skimr package (Waring et al. 2022). It skims a data set and returns, for example, a short summary, summary statistics, and missing values. Inspect the vignette and skim() the data frame.\n\n# Inspect skimr package (and vignette)\n# vignette(\"skimr\")\nskimr::skim(df)\n\nOr examine the ggpairs() function from the GGally package (Schloerke et al. 2021). It provides many extensions to create graphs (with ggplot2 see Chapter 7); and it also has functions to explore data and effects. The ggpairs() function returns a graph for a pairwise comparison of all variables. Depending on the data type, it returns bar plots, density plot, or the correlation between variables and combines all plots in one graph.\n\n# GGally: https://ggobi.github.io/ggally/\nGGally::ggpairs(df[2:5])"
  },
  {
    "objectID": "chapter_03.html#summary",
    "href": "chapter_03.html#summary",
    "title": "2  Data Exploration",
    "section": "2.4 Summary",
    "text": "2.4 Summary\nData exploration can be exciting since we explore something new. Unfortunately, it can be painful if the data is complex or messy. For this reason we used a simple and clean data, but we will start to manipulate complex(er) data and prepare messy data soon. Keep the following functions from Chapter 3 in mind:\n\nGet a glimpse of your data (dplyr::glimpse); display the structure of an object (str); and inspect the first or last parts of an object (head/tail)\nCreate a factor variable (factor); levels attributes (levels); object labels (labels)\nSimple cross table (table)\nGet a summary (summary)\nSummary statistics (min, mean, max, sd)\nCorrelation, variance and covariance (matrices) via (cor); or with the correlation package (Makowski et al. 2022)\nGraphs: Bar plots (barplot); histograms (hist), spine plot (spineplot), box plot (boxplot), scatter plot (plot), correlation matrix (corrplot::corrplot)\nPackages:\n\nThe summarytools package provides many tables: (e.g., freq, ctable)\nThe DataExplorer to visualize several variable at once: (e.g., plot_bar)\nThe effectsize package to interpret results: (e.g., interpret_r)\n\n\n\n\n\n\nBen-Shachar, Mattan S., Dominique Makowski, Daniel Lüdecke, Indrajeet Patil, and Brenton M. Wiernik. 2022. effectsize: Indices of Effect Size. https://CRAN.R-project.org/package=effectsize.\n\n\nComtois, Dominic. 2022. summarytools: Tools to Quickly and Neatly Summarize Data. https://CRAN.R-project.org/package=summarytools.\n\n\nCui, Boxuan. 2020. DataExplorer: Automate Data Exploration and Treatment. https://CRAN.R-project.org/package=DataExplorer.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nMakowski, Dominique, Brenton M. Wiernik, Indrajeet Patil, Daniel Lüdecke, and Mattan S. Ben-Shachar. 2022. Correlation: Methods for Correlation Analysis. https://CRAN.R-project.org/package=correlation.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. pillar: Coloured Formatting for Columns. https://CRAN.R-project.org/package=pillar.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "chapter_04.html#select",
    "href": "chapter_04.html#select",
    "title": "3  Data manipulation with dplyr",
    "section": "3.1 Select",
    "text": "3.1 Select\nEspecially in case of large and cluttered data, we use select() to specify which variables we work with. For example, pick only one variable such as school degree from the gss2016 data.\n\n# Select a variable\nselect(gss2016, degree)\n\n#&gt; # A tibble: 2,867 × 1\n#&gt;   degree     \n#&gt;   &lt;fct&gt;      \n#&gt; 1 Bachelor   \n#&gt; 2 High School\n#&gt; 3 Bachelor   \n#&gt; 4 High School\n#&gt; 5 Graduate   \n#&gt; # ℹ 2,862 more rows\n\n\nSelect comes with handy functions and applies the same logic as base R. For example, select several columns by providing a start (e.g., id) and endpoint (e.g., degree).\n\n# Select all variables from x to y\nselect(gss2016, id:degree) |&gt; head()\n\n#&gt; # A tibble: 6 × 6\n#&gt;      id ballot       age childs sibs       degree     \n#&gt;   &lt;dbl&gt; &lt;labelled&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;labelled&gt; &lt;fct&gt;      \n#&gt; 1     1 1             47      3 2          Bachelor   \n#&gt; 2     2 2             61      0 3          High School\n#&gt; 3     3 3             72      2 3          Bachelor   \n#&gt; 4     4 1             43      4 3          High School\n#&gt; 5     5 3             55      2 2          Graduate   \n#&gt; # ℹ 1 more row\n\n\nMaybe we need all columns except the variables shown in the last output. Ask for the opposite and insert parentheses and a minus signs to turn the selection around.\n\n# Turn around the selection\nselect(gss2016, -(id:degree)) |&gt; head()\n\n#&gt; # A tibble: 6 × 27\n#&gt;    year race  sex    region     income16 relig marital padeg\n#&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt;      &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;   &lt;fct&gt;\n#&gt; 1  2016 White Male   New Engla… $170000… None  Married Grad…\n#&gt; 2  2016 White Male   New Engla… $50000 … None  Never … Lt H…\n#&gt; 3  2016 White Male   New Engla… $75000 … Cath… Married High…\n#&gt; 4  2016 White Female New Engla… $170000… Cath… Married &lt;NA&gt; \n#&gt; 5  2016 White Female New Engla… $170000… None  Married Bach…\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 19 more variables: madeg &lt;fct&gt;, partyid &lt;fct&gt;,\n#&gt; #   polviews &lt;fct&gt;, happy &lt;fct&gt;, partners &lt;fct&gt;,\n#&gt; #   grass &lt;fct&gt;, zodiac &lt;fct&gt;, pres12 &lt;labelled&gt;,\n#&gt; #   wtssall &lt;dbl&gt;, income_rc &lt;fct&gt;, agegrp &lt;fct&gt;,\n#&gt; #   ageq &lt;fct&gt;, siblings &lt;fct&gt;, kids &lt;fct&gt;, religion &lt;fct&gt;,\n#&gt; #   bigregion &lt;fct&gt;, partners_rc &lt;fct&gt;, obama &lt;dbl&gt;, …\n\n\nThe gss2016 data does not contain variables with a running number nor other systematic variable names. However, dplyr helps to select such variables without much effort. Consider toy data with several measurements and running numbers to illustrate how we can select such variables efficiently.\n\n# A new df to illustrate\ndf &lt;- tibble(\n  measurement_1 = 1:3,\n  x1 = 1:3,\n  measurement_2 = 1:3,\n  x2 = 1:3,\n  x3 = 1:3,\n  other_variables = 1\n)\n\nSuppose we measured a variables several times and all start with an identical name (e.g., measurement_). Select all variables which start (or end) with a certain string. Thus, insert the starts_with() function and select all measurement variables.\n\n# Select variables that start with a string\nselect(df, starts_with(\"measurement\"))\n\n#&gt; # A tibble: 3 × 2\n#&gt;   measurement_1 measurement_2\n#&gt;           &lt;int&gt;         &lt;int&gt;\n#&gt; 1             1             1\n#&gt; 2             2             2\n#&gt; 3             3             3\n\n\nOr pick variables with the running number. The num_range functions needs the name (x) and the running number.\n\n# Select based on a running number\nselect(df, num_range(\"x\", 1:3))\n\n#&gt; # A tibble: 3 × 3\n#&gt;      x1    x2    x3\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     1     1\n#&gt; 2     2     2     2\n#&gt; 3     3     3     3\n\n\nThe package offers more helpers to select variables than I can possibly outline. For example, contains() checks if a variable includes a certain word; matches() let us specify search patterns (regular expression, see Chapter 10); and we can also include other functions to select variables. For example, the is.numeric function checks if an input is numeric and we can combine it with where() to select columns only where the content is numeric.\n\n# Insert a function to select variables\ngss2016 |&gt; select(where(is.numeric))\n\n#&gt; # A tibble: 2,867 × 10\n#&gt;    year    id ballot   age childs sibs  pres12 wtssall obama\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;labe&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lab&gt; &lt;labe&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2016     1 1         47      3 2     3        0.957     0\n#&gt; 2  2016     2 2         61      0 3     1        0.478     1\n#&gt; 3  2016     3 3         72      2 3     2        0.957     0\n#&gt; 4  2016     4 1         43      4 3     2        1.91      0\n#&gt; 5  2016     5 3         55      2 2     1        1.44      1\n#&gt; # ℹ 2,862 more rows\n#&gt; # ℹ 1 more variable: income &lt;dbl&gt;\n\n\nNext, we filter data but since all R outputs are large due to the gss2016 data, let us first create a smaller subset to reduce the size of the output and the length of this document.\n\n# Select a smaller subset for the rest of this tutorial\ngss2016 &lt;- select(PracticeR::gss2016, year:sex, income)"
  },
  {
    "objectID": "chapter_04.html#filter",
    "href": "chapter_04.html#filter",
    "title": "3  Data manipulation with dplyr",
    "section": "3.2 Filter",
    "text": "3.2 Filter\nUse filter() to subset the data. The dplyr filters the data and returns a new data frame depending on the specified conditions. Use one or several relational or logical operators to select observations. For example, suppose you want to analyze persons who have a bachelor’s degree only.\n\n# Apply a filter\ngss2016 |&gt;\n  filter(degree == \"Bachelor\") |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 10\n#&gt;    year    id ballot     age childs sibs  degree race  sex  \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;labell&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lab&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;\n#&gt; 1  2016     1 1           47      3 2     Bache… White Male \n#&gt; 2  2016     3 3           72      2 3     Bache… White Male \n#&gt; 3  2016    37 2           59      2 2     Bache… White Male \n#&gt; 4  2016    38 1           43      2 6     Bache… White Fema…\n#&gt; 5  2016    39 3           58      0 1     Bache… White Fema…\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: income &lt;dbl&gt;\n\n\nCan you adjust the code so that two conditions have to be fulfilled simultaneously. For example, keep only observations from adults (18 years and older) with a bachelor’s degree.\n\n# Combine several conditions\ngss2016 |&gt;\n  filter(degree == \"Bachelor\" & age &gt; 17) |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 10\n#&gt;    year    id ballot     age childs sibs  degree race  sex  \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;labell&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lab&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;\n#&gt; 1  2016     1 1           47      3 2     Bache… White Male \n#&gt; 2  2016     3 3           72      2 3     Bache… White Male \n#&gt; 3  2016    37 2           59      2 2     Bache… White Male \n#&gt; 4  2016    38 1           43      2 6     Bache… White Fema…\n#&gt; 5  2016    39 3           58      0 1     Bache… White Fema…\n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: income &lt;dbl&gt;\n\n\nAs outlined, keep your base R skills in mind when selecting or filtering data. For example, keep all degrees but exclude persons who have a Bachelor.\n\n# All degrees, but not! Bachelors\ngss2016 |&gt;\n  filter(degree != \"Bachelor\") |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 10\n#&gt;    year    id ballot     age childs sibs  degree race  sex  \n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;labell&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lab&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;\n#&gt; 1  2016     2 2           61      0 3     High … White Male \n#&gt; 2  2016     4 1           43      4 3     High … White Fema…\n#&gt; 3  2016     5 3           55      2 2     Gradu… White Fema…\n#&gt; 4  2016     6 2           53      2 2     Junio… White Fema…\n#&gt; 5  2016     7 1           50      2 2     High … White Male \n#&gt; # ℹ 1 more row\n#&gt; # ℹ 1 more variable: income &lt;dbl&gt;\n\n\nUse the operators() function from the PracticeR package when you have trouble to remember how logical and relational operators are implemented. The function inserts and runs examples via the console.\n\nPracticeR::operators(\"logical\")\n# ── Logical Operators\n# &gt; x &lt;- TRUE\n# &gt; y &lt;- FALSE\n# &gt; #Elementwise logical AND\n# &gt; x & y == TRUE\n# [1] FALSE\n# &gt; #Elementwise logical OR\n# &gt; x | y == TRUE\n# [1] TRUE\n# &gt; #Elementwise OR\n# &gt; xor(x, y)\n# [1] TRUE\n# &gt; #Logical NOT\n# &gt; !x\n# [1] FALSE\n# &gt; #In operator\n# &gt; 1:3 %in% rep(1:2)\n# [1]  TRUE  TRUE FALSE"
  },
  {
    "objectID": "chapter_04.html#mutate",
    "href": "chapter_04.html#mutate",
    "title": "3  Data manipulation with dplyr",
    "section": "3.3 Mutate",
    "text": "3.3 Mutate\nIn Chapter 4 I outline several ways to generate new variables based on observed ones. For example, raw data often contains a person’s year of birth but not their age. With mutate() we can extend the data frame and estimate such a variable. Unfortunately, the gss2016 has an age variable, but the variable does only reveal their age when the survey was conducted. To recap how mutate() works, recreate their birth year and a recent age variable, say for the year 2023.\n\n# Create birth_year and a recent (year: 2023) age variable\ngss2016 |&gt;\n  select(id, year, age) |&gt;\n  mutate(\n    birth_year = year - age,\n    age_2023 = 2023 - birth_year\n  ) |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 5\n#&gt;      id  year   age birth_year age_2023\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1  2016    47       1969       54\n#&gt; 2     2  2016    61       1955       68\n#&gt; 3     3  2016    72       1944       79\n#&gt; 4     4  2016    43       1973       50\n#&gt; 5     5  2016    55       1961       62\n#&gt; # ℹ 1 more row\n\n\nKeep in mind that you can use relational and logical operators, as well other functions (e.g., log, rankings, etc.) to generate new variables. For example, generate a logical variable that indicates whether a person was an adult (older than 17) in the year 2016. The if_else() function helps you with this job.\n\n# In theory: if_else(condition, true, false, missing = NULL)\ngss2016 |&gt;\n  select(id, year, age) |&gt;\n  mutate(adult = if_else(age &gt; 17, TRUE, FALSE)) |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 4\n#&gt;      id  year   age adult\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n#&gt; 1     1  2016    47 TRUE \n#&gt; 2     2  2016    61 TRUE \n#&gt; 3     3  2016    72 TRUE \n#&gt; 4     4  2016    43 TRUE \n#&gt; 5     5  2016    55 TRUE \n#&gt; # ℹ 1 more row\n\n\nIn terms of generating new variables, also keep the case_when() function in mind, which provides a very flexible approach. Suppose we need to identify parents with a academic background. Parents educational background has many levels or attributes in the gss2016 data, which makes a first attempt harder to apply (and we learn more about factor variables in Chapter 5). For this reason I created a smaller toy data set and I started to prepare the code. Can you complete it? The variable academic_parents is supposed to identify persons with a high educational background (education) with one or more kids. All other conditions are set to FALSE.\n\n# Data to illustrate\ndf &lt;- data.frame(\n  kids = c(0, 1, 3, 0, NA),\n  educ = c(\"high\", \"low\", \"high\", \"low\", NA)\n)\n\n# In theory: case_when(condition ~ value)\ndf |&gt;\n  mutate(academic_parents = case_when(\n    kids &gt;= 1 & educ == \"high\" ~ \"TRUE\",\n    TRUE ~ \"FALSE\"\n  ))\n\n#&gt;   kids educ academic_parents\n#&gt; 1    0 high            FALSE\n#&gt; 2    1  low            FALSE\n#&gt; 3    3 high             TRUE\n#&gt; 4    0  low            FALSE\n#&gt; 5   NA &lt;NA&gt;            FALSE"
  },
  {
    "objectID": "chapter_04.html#summarize",
    "href": "chapter_04.html#summarize",
    "title": "3  Data manipulation with dplyr",
    "section": "3.4 Summarize",
    "text": "3.4 Summarize\nThe summarize() function collapses several columns into a single row. By the way, the dplyr package understands both, British (e.g., summarise) and American English (e.g. summarize) and it’s up to you to decide which one you prefer.\nLet’s calculate the mean age of the survey participants. As outlined in Practice R, the variable has missing values which is why we need to drop them first. In Chapter 5 we will focus on this problem and we learn more about the consequences of such decisions. I already excluded missing values, can you summarize() the age?\n\n# Exclude missing values but consider the consequences (see Chapter 5)\ngss2016 &lt;- gss2016 |&gt;\n  tidyr::drop_na(age, sex)\n\n# Summarize age\ngss2016 |&gt; summarize(mean_age = mean(age))\n\n#&gt; # A tibble: 1 × 1\n#&gt;   mean_age\n#&gt;      &lt;dbl&gt;\n#&gt; 1     49.2\n\n\nThe dplyr package comes with several help functions to summarize data. For example, to count the number of observation per group (e.g., for sex), split the data by groups (group_by) and apply the n() function.\n\n# County by (sex)\ngss2016 |&gt;\n  group_by(sex) %&gt;%\n  summarize(count = n())\n\n#&gt; # A tibble: 2 × 2\n#&gt;   sex    count\n#&gt;   &lt;fct&gt;  &lt;int&gt;\n#&gt; 1 Male    1272\n#&gt; 2 Female  1585\n\n\nMoreover, compare the groups by calculating the median age instead of the mean; add the standard deviation (sd); and count the number of distinct values (n_distinct) of the degree variable.\n\n# Dplyr has more summary functions\ngss2016 |&gt;\n  group_by(sex) |&gt;\n  summarise(\n    median_age = median(age),\n    sd_age = sd(age),\n    distinct_degree = n_distinct(degree)\n  )\n\n#&gt; # A tibble: 2 × 4\n#&gt;   sex    median_age sd_age distinct_degree\n#&gt;   &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;           &lt;int&gt;\n#&gt; 1 Male           48   17.4               6\n#&gt; 2 Female         50   17.9               6\n\n\nIn the last examples we grouped the data and then collapsed it. The counterpart to group is ungroup() which we may add as a last step to disperse the data again. For example, we can estimate how old men or women are on average and add this information to the original data frame. Use mutate() instead of summarise() to see the logic behind ungroup.\n\n# Mutate ungroups the data again\ngss2016 |&gt;\n  select(id, sex, age) |&gt;\n  group_by(sex) |&gt;\n  mutate(count = round(mean(age), 2))\n\n#&gt; # A tibble: 2,857 × 4\n#&gt; # Groups:   sex [2]\n#&gt;      id sex      age count\n#&gt;   &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1 Male      47  48.3\n#&gt; 2     2 Male      61  48.3\n#&gt; 3     3 Male      72  48.3\n#&gt; 4     4 Female    43  49.8\n#&gt; 5     5 Female    55  49.8\n#&gt; # ℹ 2,852 more rows"
  },
  {
    "objectID": "chapter_04.html#arrange",
    "href": "chapter_04.html#arrange",
    "title": "3  Data manipulation with dplyr",
    "section": "3.5 Arrange",
    "text": "3.5 Arrange\nLast but not least, keep the arrange() function in mind. It is easy to apply and I don’t believe there is much to practice. However, it gives us the chance to repeat how transmute() and the between() function works.\nConsider the steps to build a restricted age sample to examine adults only. Use mutate to create a logical variable (age_filter) that indicates if a person is between 18 and 65. Furthermore, explore the difference between mutate() and transmute() if you can’t remember it.\n\n# Create a restricted analysis sample\n# between: x &gt;= left & x &lt;= right\ngss2016 |&gt;\n  transmute(age,\n    age_filter = between(age, 18, 65)\n  )\n\n#&gt; # A tibble: 2,857 × 2\n#&gt;     age age_filter\n#&gt;   &lt;dbl&gt; &lt;lgl&gt;     \n#&gt; 1    47 TRUE      \n#&gt; 2    61 TRUE      \n#&gt; 3    72 FALSE     \n#&gt; 4    43 TRUE      \n#&gt; 5    55 TRUE      \n#&gt; # ℹ 2,852 more rows\n\n\nNext, we need a filter() to restrict the sample, but how can we know that code worked? We can inspect the entire data frame with View, but we can also use arrange() to inspect if the filter was correctly applied. Sort in ascending and descending (desc) order.\n\n# Filter and arrange the data\ngss2016 |&gt;\n  transmute(age,\n    age_filter = between(age, 18, 65)\n  ) |&gt;\n  filter(age_filter == \"TRUE\") |&gt;\n  arrange(desc(age)) |&gt;\n  head()\n\n#&gt; # A tibble: 6 × 2\n#&gt;     age age_filter\n#&gt;   &lt;dbl&gt; &lt;lgl&gt;     \n#&gt; 1    65 TRUE      \n#&gt; 2    65 TRUE      \n#&gt; 3    65 TRUE      \n#&gt; 4    65 TRUE      \n#&gt; 5    65 TRUE      \n#&gt; # ℹ 1 more row\n\n\nThe dplyr package offers many functions to manipulate data and this tutorial only summarizes the main functions. Consider the cheat sheet and the package website for more information.\n\n# The dplyr website\nPracticeR::show_link(\"dplyr\", browse = FALSE)\n#&gt; [1] \"https://dplyr.tidyverse.org/\"\n\nKeep in mind that data preparation steps may appear simple, but only as long as we are not supposed to prepare data on our own. In the latter case we will often need several attempts to come up with a solution that works. Thus, be patient with yourself when your first attempts will not work. Most of the time we all need more than one shot to come up with a workable solution. In addition, we will use the package one more time to combine data in Chapter 5 and other dplyr functions will appear through the Practice R book. Thus, there will be plenty of opportunities to apply and develop your dplyr skills.\nThere are often different approaches that lead to the same result. As the artwork by Jake Clark illustrates and the Practice R info box about data manipulation approaches underlines, the subset() function from base R does essentially the same as dplyr::filter. Base R provides the most stable solution, while dplyr is more verbose and often easier to learn. Don’t perceive them as two different dialects that forces us to stick to one approach. Instead, embrace them both because you will come across different approaches if you use Google to solve a problem. Fortunately, many roads lead to Rome.\n\n\n\nArtwork by Jake Clark"
  },
  {
    "objectID": "chapter_04.html#summary",
    "href": "chapter_04.html#summary",
    "title": "3  Data manipulation with dplyr",
    "section": "3.6 Summary",
    "text": "3.6 Summary\nKeep the main dplyr functions in mind, among them:\n\nKeep rows that match a condition (filter)\nOrder rows using column values (arrange)\nKeep or drop columns using their names and types (select)\nCreate, modify, and delete columns (mutate, transmute)\nSummarize each group down to one row (summarize)\nChange column order (relocate)\nVectorized if-else (if_else)\nA general vectorized if-else (case_when)\nApply a function (or functions) across multiple columns (across)\nSelect all variables or the last variable (e.g., everything)\n\nAnd the following base functions:\n\nThe names of an object (names)\nSub-setting vectors, matrices and data frames (subset)\nApply a function over a list or vector (lapply, sapply)\nRead R code from a file, a connection or expressions (source)\n\n\n\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWickham, Hadley. 2022. forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "chapter_05.html#inspect-factors",
    "href": "chapter_05.html#inspect-factors",
    "title": "4  Prepare categorical variables",
    "section": "4.1 Inspect factors",
    "text": "4.1 Inspect factors\nSuppose we need to prepare several categorical variables, such as religion (relig) or marital status (marital), for an analysis. To inspect factors, count them with fct_count().\n\n# Count factor variable\nfct_count(df$marital)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f                 n\n#&gt;   &lt;fct&gt;         &lt;int&gt;\n#&gt; 1 Married        1212\n#&gt; 2 Widowed         251\n#&gt; 3 Divorced        495\n#&gt; 4 Separated       102\n#&gt; 5 Never Married   806\n#&gt; 6 &lt;NA&gt;              1\n\n\nOr examine the unique levels of a variable with the fct_unique() function:\n\n# How many unique levels do we observe\nfct_unique(df$marital)\n\n#&gt; [1] Married       Widowed       Divorced      Separated     Never Married\n#&gt; [6] &lt;NA&gt;         \n#&gt; Levels: Married Widowed Divorced Separated Never Married"
  },
  {
    "objectID": "chapter_05.html#change-the-order-of-levels",
    "href": "chapter_05.html#change-the-order-of-levels",
    "title": "4  Prepare categorical variables",
    "section": "4.2 Change the order of levels",
    "text": "4.2 Change the order of levels\nThe variable religion (relig) has 13 different levels. Let’s assume we want to control for the largest religious groups only in the analysis. Use the fct_infreq() function to identify how often each level appears.\n\n# fct_infreq: Reorder factor levels by frequency\nf &lt;- fct_infreq(df$relig)\nfct_count(f)\n\n#&gt; # A tibble: 14 × 2\n#&gt;    f                           n\n#&gt;    &lt;fct&gt;                   &lt;int&gt;\n#&gt;  1 Protestant               1371\n#&gt;  2 Catholic                  649\n#&gt;  3 None                      619\n#&gt;  4 Jewish                     51\n#&gt;  5 Other                      44\n#&gt;  6 Christian                  40\n#&gt;  7 Buddhism                   21\n#&gt;  8 Moslem/Islam               19\n#&gt;  9 Hinduism                   13\n#&gt; 10 Orthodox-Christian          7\n#&gt; 11 Inter-Nondenominational     7\n#&gt; 12 Other Eastern               4\n#&gt; 13 Native American             4\n#&gt; 14 &lt;NA&gt;                       18\n\n\nThe fct_infreq() sorts them in order of their frequency, but note we can also order the levels by first appearance (fct_inorder) or in a numeric order (fct_inseq). As the next console illustrates, R sorts levels alphabetically, which is clearly not always a desirable default behavior. Use the fct_inorder() to sort them by appearance.\n\n# Example factor\nf &lt;- factor(c(\"b\", \"a\", \"c\"))\nlevels(f)\n\n#&gt; [1] \"a\" \"b\" \"c\"\n\n# fct_inorder: Reorder factor levels by first appearance\nfct_inorder(f)\n\n#&gt; [1] b a c\n#&gt; Levels: b a c\n\n\nCan you still remember how to manually relevel? Use the fct_relevel() and sort the level Never Married at the second position. You can provide a vector with level names or use the after option to change the position of the level.\n\n# Relevel manually\n# f &lt;- fct_relevel(df$marital, c(\"Married\", \"Never Married\"))\nf &lt;- fct_relevel(df$marital, \"Never Married\", after = 1)\nfct_count(f)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f                 n\n#&gt;   &lt;fct&gt;         &lt;int&gt;\n#&gt; 1 Married        1212\n#&gt; 2 Never Married   806\n#&gt; 3 Widowed         251\n#&gt; 4 Divorced        495\n#&gt; 5 Separated       102\n#&gt; 6 &lt;NA&gt;              1\n\n\nSometimes we need to turn the order around. Reverse the order of the levels with fct_rev().\n\n# fct_rev: Reverse order of factor levels\nf &lt;- fct_rev(df$marital)\nfct_count(f)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f                 n\n#&gt;   &lt;fct&gt;         &lt;int&gt;\n#&gt; 1 Never Married   806\n#&gt; 2 Separated       102\n#&gt; 3 Divorced        495\n#&gt; 4 Widowed         251\n#&gt; 5 Married        1212\n#&gt; 6 &lt;NA&gt;              1"
  },
  {
    "objectID": "chapter_05.html#change-the-value-of-levels",
    "href": "chapter_05.html#change-the-value-of-levels",
    "title": "4  Prepare categorical variables",
    "section": "4.3 Change the value of levels",
    "text": "4.3 Change the value of levels\nThe relig variable has many levels and even has a category named other, since there are so many religious groups. The same logic applies the fct_other() function which collapses all levels but the one we actually need. Create a variable that includes the five largest groups only. Use the fct_other() function and tell R which variables to keep.\n\n# Create a variable with the five largest, rest are others\ndf$relig5 &lt;- fct_other(df$relig,\n  keep = c(\"Protestant\", \"Catholic\", \"None\", \"Jewish\")\n)\n\nfct_count(df$relig5)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f              n\n#&gt;   &lt;fct&gt;      &lt;int&gt;\n#&gt; 1 Protestant  1371\n#&gt; 2 Catholic     649\n#&gt; 3 Jewish        51\n#&gt; 4 None         619\n#&gt; 5 Other        159\n#&gt; 6 &lt;NA&gt;          18\n\n\nThe fct_other() function includes in the code the used levels. If we are unconcerned about this information, you can use one of the fct_lump() functions. The function picks between different methods to lump together factor levels. Nowadays the authors recommend to use one of the specific fct_lump_* functions (fct_lump_min, fct_lump_prop, fct_lump_lowfreq) as outlined in the help file. In our case, use the fct_lump_n() function to lump together the most frequent (n) ones.\n\n# Lump uncommon factor together levels into \"other\"\nf &lt;- fct_lump_n(df$relig, n = 5, other_level = \"Further groups\")\nfct_count(f)\n\n#&gt; # A tibble: 7 × 2\n#&gt;   f                  n\n#&gt;   &lt;fct&gt;          &lt;int&gt;\n#&gt; 1 Protestant      1371\n#&gt; 2 Catholic         649\n#&gt; 3 Jewish            51\n#&gt; 4 None             619\n#&gt; 5 Other             44\n#&gt; 6 Further groups   115\n#&gt; 7 &lt;NA&gt;              18\n\n\nNext, we are going to prepare the educational background. The variable degree includes several levels, as the console shows.\n\n# Count degrees\nfct_count(df$degree)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f                  n\n#&gt;   &lt;fct&gt;          &lt;int&gt;\n#&gt; 1 Lt High School   328\n#&gt; 2 High School     1461\n#&gt; 3 Junior College   216\n#&gt; 4 Bachelor         536\n#&gt; 5 Graduate         318\n#&gt; 6 &lt;NA&gt;               8\n\n\nWe already used the fct_recode() function to change factor levels by hand. The lowest category of degree is called less than high school but the text label is confusing. Recode the variable, insert the new label in back ticks to replace the old label (Lt High School).\n\n# fct_recode: Change factor levels by hand\nf &lt;- fct_recode(df$degree, `Less than high school` = \"Lt High School\")\nfct_count(f)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   f                         n\n#&gt;   &lt;fct&gt;                 &lt;int&gt;\n#&gt; 1 Less than high school   328\n#&gt; 2 High School            1461\n#&gt; 3 Junior College          216\n#&gt; 4 Bachelor                536\n#&gt; 5 Graduate                318\n#&gt; 6 &lt;NA&gt;                      8\n\n\nSuppose we want to control only if participants have a high educational background. Use the fct_collapse() function to create a binary dummy variable. The variable should indicate if a person’s educational background is low (Lt High School; High School, and Junior College) or high (Bachelor and Graduate).\n\n# Collapse factor variable\ndf$edu_dummy &lt;- fct_collapse(df$degree,\n  low = c(\n    \"Lt High School\",\n    \"High School\",\n    \"Junior College\"\n  ),\n  high = c(\"Bachelor\", \"Graduate\")\n)\n\nfct_count(df$edu_dummy)\n\n#&gt; # A tibble: 3 × 2\n#&gt;   f         n\n#&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1 low    2005\n#&gt; 2 high    854\n#&gt; 3 &lt;NA&gt;      8"
  },
  {
    "objectID": "chapter_05.html#add-or-drop-levels",
    "href": "chapter_05.html#add-or-drop-levels",
    "title": "4  Prepare categorical variables",
    "section": "4.4 Add or drop levels",
    "text": "4.4 Add or drop levels\nAs always, the forcats package has more to offer than I can outline. For example, suppose we observed the following religion variable.\n\n# New religion variable\nreligion &lt;- factor(\n  x = c(\"Protestant\", \"Jewish\", NA, NA),\n  levels = c(\"Protestant\", \"Jewish\", \"Catholic\")\n)\n\nreligion\n\n#&gt; [1] Protestant Jewish     &lt;NA&gt;       &lt;NA&gt;      \n#&gt; Levels: Protestant Jewish Catholic\n\n\nDid you notice that the variable has a level for Catholic even though we do not observe it. The fct_expand() can be used to expand levels, while the fct_drop() function helps us to get rid of unused levels.\n\n# Drop unused levels\nfct_drop(religion)\n\n#&gt; [1] Protestant Jewish     &lt;NA&gt;       &lt;NA&gt;      \n#&gt; Levels: Protestant Jewish\n\n\nFurthermore, I included missing values on purpose and the latter may have an impact on our analysis. We can make them explicit and include them as a level with fct_na_value_to_level().\n\n# Make NAs explicit\nfct_na_value_to_level(religion, level = \"Missing\")\n\n#&gt; [1] Protestant Jewish     Missing    Missing   \n#&gt; Levels: Protestant Jewish Catholic Missing"
  },
  {
    "objectID": "chapter_05.html#further-steps",
    "href": "chapter_05.html#further-steps",
    "title": "4  Prepare categorical variables",
    "section": "4.5 Further steps",
    "text": "4.5 Further steps\nChapter 5 discussed many steps to prepare data, but of course this was not an all-encompassing list. I introduced data formats and we learned how to combine data given that many official data sets are split into several files. Unfortunately, transforming and combining data can be tricky and we may introduce mistakes if we neglected to prepare and clean the data properly. Thus, it is up to you to assure that the data can be transformed (combined) and further cleaning steps might be necessary.\nInstead of re-running these steps with the gss2016 data, let us explore how the tidyr package can help with the task (Wickham and Girlich 2022). As other packages, tidyr has a cheat sheet and provides a tiny data set that lets us repeat how the functions work. For example, the table4a data is a wide data set with observations of three countries and two years.\n\n# Example wide table\nhead(table4a)\n\n#&gt; # A tibble: 3 × 3\n#&gt;   country     `1999` `2000`\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan    745   2666\n#&gt; 2 Brazil       37737  80488\n#&gt; 3 China       212258 213766\n\n\nUse the pivot_longer() function to transform the data. The long data should have a new variable for the year (via names_to) and you can give the values (values_to) to a variable named cases.\n\n# Make em longer\npivot_longer(table4a,\n  cols = 2:3, names_to = \"year\",\n  values_to = \"cases\"\n)\n\n#&gt; # A tibble: 6 × 3\n#&gt;   country     year   cases\n#&gt;   &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n#&gt; 1 Afghanistan 1999     745\n#&gt; 2 Afghanistan 2000    2666\n#&gt; 3 Brazil      1999   37737\n#&gt; 4 Brazil      2000   80488\n#&gt; 5 China       1999  212258\n#&gt; 6 China       2000  213766\n\n\nOr consider the table2 data, the variable type has two outcome types (cases and population) which underline why we should transform the data into the wide format.\n\n# Example long table\nhead(table2)\n\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year type           count\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999 cases            745\n#&gt; 2 Afghanistan  1999 population  19987071\n#&gt; 3 Afghanistan  2000 cases           2666\n#&gt; 4 Afghanistan  2000 population  20595360\n#&gt; 5 Brazil       1999 cases          37737\n#&gt; 6 Brazil       1999 population 172006362\n\n\nKeep in mind that we need to provide where the names (names_from) and the values (values_from) are coming from to transform the data.\n\n# Make it wider\npivot_wider(table2,\n  names_from = type,\n  values_from = count\n)\n\n#&gt; # A tibble: 6 × 4\n#&gt;   country      year  cases population\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 Afghanistan  1999    745   19987071\n#&gt; 2 Afghanistan  2000   2666   20595360\n#&gt; 3 Brazil       1999  37737  172006362\n#&gt; 4 Brazil       2000  80488  174504898\n#&gt; 5 China        1999 212258 1272915272\n#&gt; 6 China        2000 213766 1280428583\n\n\n\nI introduced these data sets because tidyr offers such simple examples in the cheat sheet that demonstrates how we can transform data. In addition, the copycat package has the code snippets from the tidyverse cheat sheets included. As the animation shows, it only takes a few seconds to insert these examples via the add-in. Start with such a simple example if you do not transform and combine data on a regular basis. After you made sure that the code works, adjust it for your purpose, but be careful how the data is transformed.\n\nThe same applies if you need to combine data. The dplyr also offers a small data set to practice mutating joins (Wickham et al. 2022). The band_members data includes names from members of two different music bands; and the band_instruments data includes their instruments.\n\n# Small data to recapture the join_* functions\nband_members\n\n#&gt; # A tibble: 3 × 2\n#&gt;   name  band   \n#&gt;   &lt;chr&gt; &lt;chr&gt;  \n#&gt; 1 Mick  Stones \n#&gt; 2 John  Beatles\n#&gt; 3 Paul  Beatles\n\nband_instruments\n\n#&gt; # A tibble: 3 × 2\n#&gt;   name  plays \n#&gt;   &lt;chr&gt; &lt;chr&gt; \n#&gt; 1 John  guitar\n#&gt; 2 Paul  bass  \n#&gt; 3 Keith guitar\n\n\nUse one of the join function (e.g., inner_join, full_join) to combine the data.\n\n# Mutating joins\nband_members |&gt; inner_join(band_instruments, by = \"name\")\n\n#&gt; # A tibble: 2 × 3\n#&gt;   name  band    plays \n#&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n#&gt; 1 John  Beatles guitar\n#&gt; 2 Paul  Beatles bass\n\nband_members |&gt; full_join(band_instruments, by = \"name\")\n\n#&gt; # A tibble: 4 × 3\n#&gt;   name  band    plays \n#&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n#&gt; 1 Mick  Stones  &lt;NA&gt;  \n#&gt; 2 John  Beatles guitar\n#&gt; 3 Paul  Beatles bass  \n#&gt; 4 Keith &lt;NA&gt;    guitar\n\n# Further joins:\n# band_members |&gt; left_join(band_instruments)\n# band_members |&gt; right_join(band_instruments)\n\n\nFinally, one last word about missing values: make sure you explore the data before you run an analysis, but don’t neglect to inspect missing and implausible values as well. The naniar package has a lot to offer for this task and of course I did not introduce everything it is capable of in Chapter 5. For example, we used the vis_miss() function to visualize missing values, but not the amount of missing values. Give the gg_miss_var() function a try. It returns a lollipop chart to visualize the amount of missing values. To display percentages, set the show_pct option to TRUE.\n\n# Visualize the amount of missing values\nlibrary(naniar)\ngg_miss_var(df, show_pct = TRUE)"
  },
  {
    "objectID": "chapter_05.html#summary",
    "href": "chapter_05.html#summary",
    "title": "4  Prepare categorical variables",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nIn addition to the discussed content, keep the following R functions and packages in mind:\n\nImport data with different packages. For example:\n\nCSV with the readr package (Wickham, Hester, and Bryan 2022)\nExcel with the readxl package (Wickham and Bryan 2022)\nSPSS or Stata with the haven package (Wickham, Miller, and Smith 2022)\n\nConvert objects into numeric (character) vectors (as.numeric, as.character)\nRename columns (dplyr::rename)\nCleans names of an object (janitor::clean_names: Firke 2021)\nCombine data:\n\nPivot data from long to wide (tidyr::pivot_wider)\nPivot data from wide to long (tidyr::pivot_longer)\nMutating joins (dplyr::inner_join, left_join, right_join, full_join)\nFiltering joins (dplyr::semi_join, anti_join)\nSet pperations (base::union, intersect, setdiff, setequal)\n\nMissing (and implausible) values:\n\nThe naniar package and its function to explore missing values (e.g., n_miss, n_complete, vis_miss)\nCheck if something is not available (e.g., base::is.na)\nConvert values to NA (dplyr::na_if)\nDrop rows containing missing values (tidyr::drop_na)\nReplace NAs with specified values (tidyr::replace_na)\n\n\n\n\n\n\nFirke, Sam. 2021. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWickham, Hadley. 2022. forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, and Jennifer Bryan. 2022. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2022. haven: Import and Export SPSS, Stata and SAS Files. https://CRAN.R-project.org/package=haven."
  },
  {
    "objectID": "chapter_06.html#estimate-a-linear-regression-analysis",
    "href": "chapter_06.html#estimate-a-linear-regression-analysis",
    "title": "5  Analyze data",
    "section": "5.1 Estimate a linear regression analysis",
    "text": "5.1 Estimate a linear regression analysis\nI used data for teaching purposes to introduce a linear regression analysis in Practice R. This made it possible to focus on the code and its implementation; we did not explore the data, there was no need to clean the data, prepare variables, or deal with missing values. Such steps are necessary to analyze data and the process is not linear: We start to explore the data, we prepare variables, and run a first analysis. However, often we need to circle back to improve the model due to different reasons (e.g. to include control variables, check on implausible values, etc.). Thus, the first estimation results are preliminary and may substantially change during the course of the model development.\nSo, we need to explore the variable first. Suppose we examine the gender wage gap: how large is the effect of sex on income? Explore the distribution of each variable. This gives us an overview how many men and women we observe and whether we may transform the outcome variable in a later step. I already adjusted the graphical parameters (par) to put the two graphs next to each other (mfrow creates one row and two columns). Create a bar plot and a histogram to examine the variables.\n\n# Count sex\ncount_sex &lt;- dplyr::count(df, sex)\n\n# Plot two graphs\npar(mfrow = c(1, 2))\nbarplot(n ~ sex, data = count_sex)\nhist(df$income)\n\n\n\n\nWe may run a first analysis after we have explored the data, cleaned, and prepared the variables. Use the lm() function to estimate a linear regression analysis. The function needs the data and a formula (y ~ x1) to estimate the effect of sex on income.\n\n# The lm function\nlm(income ~ sex, data = df)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = income ~ sex, data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    sexFemale  \n#&gt;     17.7642      -0.7323\n\n\nSince income is not measured on a numeric scale, this coefficient is hard to interpret, but in accordance with theoretical expectations, females have a lower income. The summary() function helps us with the interpretation of the model. It returns the estimated coefficients, R², and further information about the model. In addition, add a second variable with a plus sign (+) and examine whether the educational background (degree_num) mediates the effect.\n\n# The summary function\nsummary(lm(income ~ sex + degree_num, data = df))\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = income ~ sex + degree_num, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.8416  -2.7738   0.9904   3.7014  11.2262 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 10.06283    0.39773  25.300  &lt; 2e-16 ***\n#&gt; sexFemale   -0.83192    0.21199  -3.924 8.92e-05 ***\n#&gt; degree_num   0.69287    0.03285  21.090  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.375 on 2590 degrees of freedom\n#&gt;   (274 observations deleted due to missingness)\n#&gt; Multiple R-squared:   0.15,  Adjusted R-squared:  0.1493 \n#&gt; F-statistic: 228.5 on 2 and 2590 DF,  p-value: &lt; 2.2e-16\n\n\nApparently, the wage gap can’t be explained by the educational background of the participants since sex has a significant effect. Use the predict() function to apply the model. I have already saved the model and I created a new data frame with example values. Apply the model and predict how income changes if degree_num increases; or examine how predicted values differ between male and female participants. Predicting values give you a better intuition about the model.\n\n# The model\nmodel &lt;- lm(income ~ sex + degree_num, data = df)\n\n# Generate example data\nnew_data &lt;- data.frame(\n  sex = c(\"Female\", \"Male\"),\n  degree_num = c(10, 10)\n)\n\n# Apply the model with predict(model, data)\npredict(model, new_data)\n\n#&gt;        1        2 \n#&gt; 16.15957 16.99149\n\n\nFinally, keep in mind that the effectsize package helps us to interpret model parameters such as R² (Ben-Shachar et al. 2022). I have saved the summary as sum_model. Can you extract R² (r.squared) from the latter and interpret it with interpret_r2() function. As default, it uses the Cohen’s rules to interpret the effect size.\n\n# Assign summary of the model\nsum_model &lt;- summary(model)\n\n# Interpret R2\neffectsize::interpret_r2(sum_model$r.squared, rules = \"cohen1988\")\n\n#&gt; [1] \"moderate\"\n#&gt; (Rules: cohen1988)"
  },
  {
    "objectID": "chapter_06.html#develop-the-model",
    "href": "chapter_06.html#develop-the-model",
    "title": "5  Analyze data",
    "section": "5.2 Develop the model",
    "text": "5.2 Develop the model\nAs outlined in Chapter 6, we develop models step by step. We start simple with a bivariate model. We include control variables to inspect how our estimation results change; we examine whether interaction effects mediate the effect; and to which extent an effect is linear. This is not an all-encompassing list, but developing a model step by step implies that we need to compare models to see how the estimation results change between steps. For this purpose we need tables and visualization to compare the estimated models.\nWe already started to develop a model as we included a second independent variable, but our approach made it hard to comprehend how the estimations results change if we add (drop) a variable. Use the huxreg() function from the huxtable package to compare models (Hugh-Jones 2022).\n\n# Models\nm1 &lt;- lm(income ~ sex, data = df)\nm2 &lt;- lm(income ~ sex + degree_num, data = df)\n\n# Develop models step by step\nhuxtable::huxreg(m1, m2)\n\n\n\n\n(1)(2)\n\n(Intercept)17.764 ***10.063 ***\n\n(0.169)   (0.398)   \n\nsexFemale-0.732 ** -0.832 ***\n\n(0.230)   (0.212)   \n\ndegree_num        0.693 ***\n\n        (0.033)   \n\nN2596        2593        \n\nR20.004    0.150    \n\nlogLik-8256.549    -8038.689    \n\nAIC16519.099    16085.377    \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nIn addition, use dot-and-whisker plots to compare model graphically. The plot_summs() function from the jtools package only needs the model names (Long 2022).\n\n# jtools returns a dot-and-whisker plot\njtools::plot_summs(m1, m2)\n\n\n\n\nNow that we have established the framework to develop models, let us inspect how we can examine non-linear effects, transform variables, and include interaction effects. Finally we need to check how model changes affect the performance of the model.\nWe have already applied to lm() function when we first created a scatter plot. Since we assume a linear relationship, we should start to examine the effect with a scatter plot in the case of a numerical independent variable. As outlined in Chapter 3, we may insert a regression line with abline and the lm() function. For example, consider the scatter plot for the effect of age on income.\n\n# Create a scatter plot\nplot(y = df$income, x = df$age)\nabline(lm(income ~ age, data = df), col = \"red\")\n\n\n\n\nIt seems though that both variables are not or only weakly related. Does this mean that we are supposed to stop here since there is no (large) effect? A linear regression assumes a linear effect, but the effect of age on income might not be linear. For example, create a squared age variable and including it in second model to examine if age has a non-linear effect. By including a squared variable for age, we can estimate if the effect increases (decreases) for older people.\n\n# Make a squared age variable\ndf$age_sqr &lt;- df$age^2\n\n# Compare models\nm1 &lt;- lm(income ~ age, data = df)\nm2 &lt;- lm(income ~ age + age_sqr, data = df)\nhuxtable::huxreg(m1, m2)\n\n\n\n\n(1)(2)\n\n(Intercept)17.066 ***10.421 ***\n\n(0.343)   (0.880)   \n\nage0.006    0.305 ***\n\n(0.007)   (0.037)   \n\nage_sqr        -0.003 ***\n\n        (0.000)   \n\nN2589        2589        \n\nR20.000    0.026    \n\nlogLik-8238.575    -8205.475    \n\nAIC16483.151    16418.951    \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWe may transform the outcome variable to increase the model fit as well. In the case of income, we often observe many people with little or average income while the amount of people with of a very high income is low. In such a case a logarithm of the income may help to increase the model fit. Keep in mind that the interpretation of the coefficient will change if we transform the variables. Regardless of the interpretation, the transformer() function shows what the distribution of a numerical variable would look like (e.g. log) if you transform it.\n\n# Transform a numerical variable\nPracticeR::transformer(df$income)\n\n\n\n\nNext, we examine interaction effects: I estimated a model with an interaction effect between happy and sex. Certainly, I only included it to repeat how this works, but it implies that the effect of happiness on income is moderated by sex. Regardless of my ad-hoc hypothesis, visualize the effect with cat_plot() function from the interactions package (Long 2021); it needs the model, the name of the predictor (pred) and the moderator variable (modx). As the plots shows, there is no significant interaction effect.\n\n# Interaction of two categorical variables\nlibrary(interactions)\nm3 &lt;- lm(income ~ happy * sex, data = df)\n\n# cat_plot for categorical predictors\ncat_plot(m3, pred = happy, modx = sex)\n\n\n\n\nSuppose we believe there is an interaction between sex and education. We may expect that male participants gain much more advantages from education than female participants. Use the interact_plot() function with the predictor variable (pred) and the moderator (modx) variable. The interval option shows the confidence interval and we can see the overlap.\n\n# Interaction model\nm3 &lt;- lm(income ~ sex * degree_num, data = df)\n\n# Interaction between sex*degree_num\ninteract_plot(m3,\n  pred = degree_num, modx = sex,\n  interval = TRUE, plot.points = FALSE\n)\n\n\n\n\nFinally, keep the performance package in mind when developing models (Lüdecke et al. 2022). Check how the performance changes if you insert a non-linear parameter, include interaction terms or if you compare different model specifications. The compare_performance() function returns several performance indicators and it even creates a radar plot if we assign and plot the results.\n\n# Compare performance\nlibrary(performance)\nperformance_models &lt;- compare_performance(m1, m2,\n  metrics = c(\"AIC\", \"BIC\", \"R2_adj\")\n)\n\n# Compare performance\nperformance_models\n\n#&gt; Comparison of Model Performance Indices\n\n#&gt;Name | Model |   AIC (weights) |   BIC (weights) |  R2 (adj.)\n#&gt;-------------------------------------------------------------\n#&gt;m1   |    lm | 16483.2 (&lt;.001) | 16500.7 (&lt;.001) | -8.117e-05\n#&gt;m2   |    lm | 16419.0 (&gt;.999) | 16442.4 (&gt;.999) |      0.025\n\n\n# Plot results\nplot(performance_models)"
  },
  {
    "objectID": "chapter_06.html#improve-the-model",
    "href": "chapter_06.html#improve-the-model",
    "title": "5  Analyze data",
    "section": "5.3 Improve the model",
    "text": "5.3 Improve the model\nThere are more steps to develop and improve the model. Up to this point we developed the model from a theoretical point of view: we checked if variables interact with each other or in case of a non-linear effect. There is still much room for improvement after we worked off theoretical points. At least we should be aware about the assumptions of a linear regression analysis and the packages that can help us to address such concerns. So, what shall we do if we finalized the first model(s)?\n\n# Final model(s)\nm_all &lt;- lm(income ~ sex + degree_num, data = df)\n\nI introduce the performance package because it gives you a quick overview about potential violations. First, the check_model() returns an overview with several plots to check the model assumptions.\n\n# Get a quick overview\ncheck_model(m_all)\n\nSecond, the package has several check_* functions to examine assumptions individually. For example, what about multicollinearity and heteroscedasticity?\n\n# multicollinearity\ncheck_collinearity(m_all)\n\n#&gt; Check for Multicollinearity\n\n#&gt; Low Correlation\n\n#&gt;       Term  VIF  VIF 95% CI Increased SE Tolerance\n#&gt;        sex 1.00 [1.00, Inf]         1.00      1.00\n#&gt; degree_num 1.00 [1.00, Inf]         1.00      1.00\n#&gt;\n#&gt; Tolerance 95% CI\n#&gt;    [0.00, 1.00]\n#&gt;     [0.00, 1.00]\n\n\n# check_heteroscedasticity\ncheck_heteroscedasticity(m_all)\n\n#&gt; Warning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nThe last function runs a statistical test to check on the assumptions; in the case of heteroscedasticity we can apply the Breusch & Pagan (1979) test (bptest), which runs in the background. The lmtest package gives you access to such statistical tests (Hothorn et al. 2022).\n\n# Breusch & Pagan test (1979)\nlmtest::bptest(m1)\n\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  m1\n#&gt; BP = 3.2195, df = 1, p-value = 0.07277\n\n\nThe error of our model is heteroscedastic and the estimatr package runs a regression with (cluster) robust standard errors to address this point (Blair et al. 2022). Run a regression with the lm_robust() function and adjust the type of standard errors with the se_type option.\n\n# Robust standard errors\nlibrary(estimatr)\nrobust_model &lt;- lm_robust(income ~ age,\n  data = df,\n  se_type = \"stata\"\n)\n\nsummary(robust_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm_robust(formula = income ~ age, data = df, se_type = \"stata\")\n#&gt; \n#&gt; Standard error type:  HC1 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)  CI Lower CI Upper   DF\n#&gt; (Intercept) 17.066368   0.348788 48.9305   0.0000 16.382436 17.75030 2587\n#&gt; age          0.005891   0.006638  0.8875   0.3749 -0.007125  0.01891 2587\n#&gt; \n#&gt; Multiple R-squared:  0.0003053 , Adjusted R-squared:  -8.117e-05 \n#&gt; F-statistic: 0.7876 on 1 and 2587 DF,  p-value: 0.3749\n\n\nFinally, one last word about the visualization of regression results. The jtools package provides convenient solutions to create dot-and-whisker plots; and the dotwhisker package lets us customize the graph (Solt and Hu 2021). For this purpose I introduce the package, but this does not mean that we have to build a long and complicated code from the ground up each time we need an individual dot-and-whisker plot. In the next chapter we learn more about ggplot2 which will boost your visualization skills and in a later step we will create functions to create plots efficiently (Wickham et al. 2022).\nThe same applies to the dotwhisker package. Once you have built a graph, you can build your own function to create such plots. Don’t let complicated code scare you off, we’ll soon work on strategies how to create plots without the trouble of memorizing complex code. For example, I created a function called visualize_model() which rebuilds the complicated code to create a dot-and-whisker plot from Chapter 6. However, it only needs the models and the names for each predictor to create the plot.\n\n# visualize_model() runs dotwhisker in the background\nvisualize_model(m_all, p1 = \"Sex\", p2 = \"Education\")"
  },
  {
    "objectID": "chapter_06.html#summary",
    "href": "chapter_06.html#summary",
    "title": "5  Analyze data",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nKeep the following R functions and packages in mind:\n\nFitting linear models (lm)\nModel predictions (predict)\nInterpret coefficient of determination (effectsize::interpret_r2)\nReorder levels of factor (relevel)\nCreate a huxtable to display model output (huxtable::huxreg)\nPlot regression summaries (jtools::plot_summs)\nPlot interaction effects in regression models (e.g., interactions::interact_plot)\nThe performance package and its functions to examine the performance of a model.\n\nCompute the model’s R2 ( r2)\nCompare performance of different models (compare_performance)\nVisual check of model assumptions (e.g.,check_model, check_outliers, check_heteroscedasticity)\n\nTransform a numerical input (PracticeR::transformer)\nExport regression summaries to tables (jtools::export_summs)\nOLS with robust standard errors (estimatr::lm_robust)\nCreate fine tuned dot-and-whisker plots API with the dotwhisker package\n\n\n\n\n\nBen-Shachar, Mattan S., Dominique Makowski, Daniel Lüdecke, Indrajeet Patil, and Brenton M. Wiernik. 2022. effectsize: Indices of Effect Size. https://CRAN.R-project.org/package=effectsize.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2022. estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nHothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint Cummins. 2022. lmtest: Testing Linear Regression Models. https://CRAN.R-project.org/package=lmtest.\n\n\nHugh-Jones, David. 2022. huxtable: Easily Create and Style Tables for LaTeX, HTML and Other Formats. https://CRAN.R-project.org/package=huxtable/.\n\n\nLong, Jacob A. 2021. interactions: Comprehensive, User-Friendly Toolkit for Probing Interactions. https://CRAN.R-project.org/package=interactions.\n\n\n———. 2022. jtools: Analysis and Presentation of Social Scientific Data. https://CRAN.R-project.org/package=jtools.\n\n\nLüdecke, Daniel, Dominique Makowski, Mattan S. Ben-Shachar, Indrajeet Patil, Philip Waggoner, and Brenton M. Wiernik. 2022. performance: Assessment of Regression Models Performance. https://CRAN.R-project.org/package=performance.\n\n\nSolt, Frederick, and Yue Hu. 2021. dotwhisker: Dot-and-Whisker Plots of Regression Results. https://CRAN.R-project.org/package=dotwhisker.\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2."
  },
  {
    "objectID": "chapter_07.html#order-the-data",
    "href": "chapter_07.html#order-the-data",
    "title": "6  Visualize data",
    "section": "6.1 Order the data",
    "text": "6.1 Order the data\nSuppose you created a bar graph to examine cars and their manufacturer (mpg$manufacturer). The data is not important, but we need to learn how to order the levels of a categorical variable. As the next console illustrates, the displayed information is difficult to perceive because the bars are all mixed up. Adjust the levels of a factor variable manually or use the fct_infreq() function from the forcats package to order the data by frequency (Wickham 2022).\n\n#Simple bar graph\np1 &lt;- ggplot(data=mpg, aes(x=manufacturer)) + \n  geom_bar()\n\n#Order the data\np2 &lt;- ggplot(data=mpg, aes(x=fct_infreq(manufacturer))) + \n  geom_bar()\n\np1 + p2\n\n\n\n\nIt is our job is to make the graph and its insights accessible. The example underlines that we need to structure and present the data in a way that leverages the message. The last graph also illustrates that there are many group levels making it difficult to depict them all in one graph even if we ordered the data. Moreover, look at the labels, they are not vertically aligned which makes it hard to read. Remember, the forcats package offers many functions to manipulate factor variables. For example, display only the five largest groups with the fct_lump function and use the coord_flip() function to turn around the axes to align the labels vertically.\n\n# Lump levels with fct_lump\nmpg$manufacturer_max &lt;- fct_lump(mpg$manufacturer, n = 5)\n\n# Left: Plot less levels\np1 &lt;- ggplot(data = mpg, aes(x = fct_infreq(manufacturer_max))) +\n  geom_bar()\n\n# Right: Flip axes\np2 &lt;- ggplot(data = mpg, aes(x = fct_infreq(manufacturer_max))) +\n  geom_bar() +\n  coord_flip()\n\np1 + p2\n\n\n\n\nTo order the data is important, regardless of the graph created. For example, suppose you examine car consumption (mpg$hwy) for different classes of cars (mpg$class) with a box plot. Look at the unsorted plot, can you tell me which level has the highest mean? It is complicated to compare groups without a useful order. Try to apply the fct_reorder() function, because it lets us reorder the class variable by its consumption (hwy).\n\n# A basic plot\np1 &lt;- ggplot(mpg, aes(hwy, class)) +\n  geom_boxplot()\n\n# Use fct_reorder to sort class by their consumption\np2 &lt;- ggplot(mpg, aes(hwy, fct_reorder(class, hwy))) +\n  geom_boxplot()\n\np1 + p2\n\n\n\n\nWe therefore are supposed to order the data and communicate in a coherent way, otherwise the audience may get confused. There are however additional pitfalls when it comes to box plots."
  },
  {
    "objectID": "chapter_07.html#boxplot-pitfalls",
    "href": "chapter_07.html#boxplot-pitfalls",
    "title": "6  Visualize data",
    "section": "6.2 Boxplot pitfalls",
    "text": "6.2 Boxplot pitfalls\nI generated fake data with a group and an outcome variable to illustrate the main concerns against box plots.\n\n# Some fake data\nglimpse(data)\n\n#&gt; Rows: 615\n#&gt; Columns: 2\n#&gt; $ group   &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n#&gt; $ outcome &lt;dbl&gt; 10.016980, 11.203247, 10.053949, 12.035642, 11.698082, 12.8889…\n\n\nSay you estimated a box plot to examine the differences between the groups. At first glance there seems to be a large differences between groups as the first box plot reveals, but are we comparing on fair grounds? See what happens if you add a geom_jitter(). It displays observations with points, but compared to a geom_point it adds a small amount of random variation to reduce over plotting.\n\n#A Basic geom_boxplot\np1 &lt;- ggplot(data, aes(x = group, y = outcome)) +\n  geom_boxplot()\n\n#Add a geom_jitter(color, size, alpha)\np2 &lt;- ggplot(data, aes(x = group, y = outcome)) +\n  geom_boxplot() +\n  geom_jitter(color = \"#d62828\", \n              size = 0.5, \n              alpha = 0.6)\n\np1 + p2\n\n\n\n\nWe are comparing three different groups, but the amount of observations are unevenly distributed between the groups and we hardly observe any from group C. This becomes visible when using geom_jitter() to add observations, compared to geom_boxplot() which does not display the data. A box plot disguises such problems which is obviously a serious concern.\nThe geom_jitter already improved the graph, what else can we do to fulfill the guiding principles of visualization. For example, include the sample size in the graph to make our reader conscious about the problem. The next steps are a bit trickier to apply: Estimate the sample size per group and assign the results. Use the dplyr::n() function to count observations, but you will need to group the data first (Wickham, François, et al. 2022).\n\n# Estimate sample_size (n) per group\nsample_size &lt;- data |&gt;\n  dplyr::group_by(group) |&gt;\n  dplyr::summarize(num = dplyr::n())\n\nsample_size\n\n#&gt; # A tibble: 3 × 2\n#&gt;   group   num\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A       100\n#&gt; 2 B       500\n#&gt; 3 C        15\n\n\nTo include the sample size in the graph, we need to combine the group label and the sample size. We can paste text strings together with the paste (and paste0) function, as the next console illustrates. It returns text strings which we can include in the graph.\n\n# Concatenate Strings with paste (and paste0)\npaste(sample_size$group, \"has N =\", sample_size$num, \" observations.\")\n\n#&gt; [1] \"A has N = 100  observations.\" \"B has N = 500  observations.\"\n#&gt; [3] \"C has N = 15  observations.\"\n\n\nFirst, combine both data sets with a left_join(). Second, create a new variable to add the text label. Use the paste function to paste the text label, but also add a new line (\\n) to separate the group name and the text to display the sample size (num).\n\n# Join data and mutate with text labels for group_N\ndata &lt;- data |&gt;\n  dplyr::left_join(sample_size) |&gt;\n  dplyr::mutate(group_N = paste0(group, \"\\n\", \"N=\", num))\n\nhead(data)\n\n#&gt;   group  outcome num  group_N\n#&gt; 1     A 10.01698 100 A\\nN=100\n#&gt; 2     A 11.20325 100 A\\nN=100\n#&gt; 3     A 10.05395 100 A\\nN=100\n#&gt; 4     A 12.03564 100 A\\nN=100\n#&gt; 5     A 11.69808 100 A\\nN=100\n#&gt; 6     A 12.88894 100 A\\nN=100\n\n\nNow we can use the new variable (group_N) as x and include the sample size. It goes without saying that there are more ways to improve a box plot (and to include text). For example, we can use a geom_violin() to examine the distribution, as the second plot on the right side shows.\n\n#Use the new variable group_N as x\np1 &lt;- ggplot(data, aes(x = group_N, y = outcome)) +\n  geom_boxplot()+\n  geom_jitter(color = \"#d62828\", \n              size = 0.5, \n              alpha = 0.6)\n\n#A violin plot and stat_summary\np2 &lt;- ggplot(data, aes(x = group, y = outcome)) +\n  geom_violin(width=0.6, alpha=0.8)+\n  stat_summary(fun = \"median\", color = \"red\", \n               size = 1.5, geom = \"point\")+\n  stat_summary(fun.data = return_stats, \n               geom = \"text\", \n               size = 2, fontface = \"bold\",\n               hjust = 0.5, vjust = 0.9) \n\np1 + p2\n\n\n\n\nAs the right plot shows, I used the stat_summary() function twice to include further statistics. First, I used the function to display the median of each group. Second, I used a function (return_stats) that returns the statistics and finally the stat_summary() function which includes them as text in the plot. The latter approach is more flexible but also more complicated than the first approach. The next console shows how the function works and we will learn more about the geom_text at the end of this tutorial.\n\n\nCode\n# The return_stats function\nreturn_stats &lt;- function(y) {\n  return(data.frame(\n    value = max(y) * 1.2,\n    label = paste(\n      \"N =\", length(y), \"\\n\",\n      \"Mean =\", round(mean(y), 2), \"\\n\",\n      \"Median =\", round(median(y), 2), \"\\n\"\n    )\n  ))\n}\n\nreturn_stats(data$outcome)\n\n\n#&gt;      value                                        label\n#&gt; 1 33.03453 N = 615 \\n Mean = 13.09 \\n Median = 12.91 \\n\n\n\nRegardless of the approach, keep in mind that a box plot does not show the data nor does it display the distribution. Compared to that, the geom_jitter() displays the data and the violin plot reveals the underlying distribution.\nTo calculate the sample size or other statistics seems a bit awkward if you are not used to customized plots. Fortunately, there are further ggplot2 extension package that help us with this task. For example, the see package has a geom_violindot() function, which combines a violin with a dot plot. The latter makes it convenient to inspect the sample size and the distribution (Lüdecke et al. 2022). Add the geom, fill the dots black (via fill_dots); and find a reasonable size for the dots via size_dots option.\n\n# The see package adds a geom_violindot\nlibrary(see)\nggplot(data, aes(x = group, y = outcome, fill = group)) +\n  geom_violindot(fill_dots = \"black\", size_dots = 5) +\n  scale_fill_material_d(palette = \"contrast\")\n\n\n\n\nOr consider the ggstatsplot package: As the result from the ggbetweenstats() function shows, the package automatically adds statistical details to the graph. In our case, it combines box and violin plots to compare the outcome between the subjects (Patil 2023).\n\n# The ggstatsplot package\nlibrary(ggstatsplot)\nggbetweenstats(data, group, outcome) +\n  theme_minimal(base_size = 10)"
  },
  {
    "objectID": "chapter_07.html#the-spaghetti-plot",
    "href": "chapter_07.html#the-spaghetti-plot",
    "title": "6  Visualize data",
    "section": "6.3 The spaghetti plot",
    "text": "6.3 The spaghetti plot\nAnother classic visualization pitfall is the spaghetti plot. Essentially it is a line graph with too many lines and colors which is why we cannot see what is going on. We can create a spaghetti plot with the babynames package and the corresponding data (Wickham 2021). The package contains names of newborn babies in the US and includes proportion for a long period (1880-2017). Suppose we examine how the most popular male names have been developed over time. I have already prepared the data to identify the most popular male names (Top 10: name_pop).\n\n# The Top 10 male names\nname_pop\n\n#&gt;  [1] \"James\"       \"Michael\"     \"Robert\"      \"John\"        \"David\"      \n#&gt;  [6] \"William\"     \"Christopher\" \"Richard\"     \"Mark\"        \"Jason\"\n\n\nTo visualize how often these names appear, we need to apply a filter to get only male babynames and to filter the data for the Top 10 names.\n\n# Get male baby names for the Top 10 names\nbabynames_df &lt;- babynames %&gt;%\n  filter(sex == \"M\" & name %in% name_pop)\n\nNext, visualize the data with a line plot (geom_line). Use year as x, n as y, and name as group and color aesthetic.\n\n# Plot\nbabynames_df %&gt;%\n  ggplot(aes(x = year, y = n, group = name, color = name)) +\n  geom_line() +\n  scale_color_viridis(discrete = TRUE) +\n  ggtitle(\"A spaghetti chart example\")\n\n\n\n\nWhat a confusing graph: single lines look like spaghettis and we can’t see how often each name was used over the time. How can we improve the spaghetti plot? You are already familiar with a simple, but powerful solution. Apply a facet_wrap() and split the graph in subplots.\n\n# Split with facet_wrap\nggplot(babynames_df, aes(x = year, y = n, group = name)) +\n  geom_line() +\n  facet_wrap(name ~ ., nrow = 2)\n\n\n\n\nThere is still room for further improvement: We could - for example - to draw all lines in gray and highlight for each facet the corresponding line in a different color. First, we need to create a copy of the name variable (facet_names), which we will use to facet the graph.\n\n# Copy the names column\nbabynames_df$facet_names &lt;- babynames_df$name\n\nNext, I prepared the geom_line() to create a spaghetti plot one more time with gray lines only, as the first plot on the left side shows. However, see what happens if you add the facet_wrap() function and the facet_names variable to split the graph.\n\np1 &lt;- ggplot(babynames_df, aes(x=year, y=n)) +\n  geom_line(data = babynames_df %&gt;% select(-facet_names), \n            aes(group=name), \n            color=\"grey\", \n            linewidth=0.5, \n            alpha=0.5)\n\np2 &lt;- ggplot(babynames_df, aes(x=year, y=n)) +\n  geom_line(data = babynames_df %&gt;% select(-facet_names), \n            aes(group=name), \n            color=\"grey\", \n            linewidth=0.5, \n            alpha=0.5)+\n  theme_minimal(base_size = 8)+\n  facet_wrap(facet_names ~ ., nrow = 2)\n  \np1 + p2\n\n\n\n\nAs the second plot show, the new variable gives us the chance to create subplot for each name, but all lines are still included if we use the copy. Next, use a second geom_line() for the overlay. Insert the name as a color aesthetic, which will make a comparison easier. Moreover, give the overlaying line a distinct color and adjust its size with linewidth.\n\n#add another geom_line as overlay\nfinal_plot &lt;- ggplot(babynames_df, aes(x=year, y=n)) +\n  geom_line(data = babynames_df %&gt;% select(-facet_names), \n            aes(group=name), \n            color=\"grey\", \n            linewidth=0.5, \n            alpha=0.5) +\n  theme_minimal(base_size = 10)+\n  facet_wrap(facet_names ~ ., nrow = 2)+\n  geom_line(aes(color=name), color=\"darkred\", linewidth=0.75)\n\nfinal_plot\n\n\n\n\nWe focused on ggplot2, because we need a print version to visualize data in applied empirical research. However, we could also make the last plot interactive to untangle the spaghetti plot. For example, Highcharts is a JavaScript software library to create interactive charts and I used the highcharter package to create a responsive HTML version of the spaghetti plot (Kunst 2022). The next console shows the code for an improved version of the graph with the highcharter package.\n\n\n\n\n\nThe highcharter package\n\n\n\n\nLearning a new package and creating interactive graphs might be too far reaching in the beginning, just keep in mind that such possibilities exits. And in this case it is not even necessary to learn a new package to make the graph interactive, because the plotly package can create an interactive version for many standard graphs that are made with ggplot2 (Sievert et al. 2022). Plotly is a JavaScript library to visualize data and can convert a ggplot2 object into a plotly chart.\n\n\n\n\n\nThe plotly package\n\n\n\n\nConsider reading Interactive web-based data visualization with R, plotly, and shiny by Carson Sievert if you want to improve your interactive visualization skills\n\n# Interactive web-based data visualization with R, plotly, and shiny\nPracticeR::show_link(\"plotly\")\n\nInstead of learning more about interactive visualization techniques, the last pitfall is not a flaw, it is a principle and an important advice."
  },
  {
    "objectID": "chapter_07.html#clutter",
    "href": "chapter_07.html#clutter",
    "title": "6  Visualize data",
    "section": "6.4 Clutter",
    "text": "6.4 Clutter\nEdward Tufte underlines: “Clutter and confusion are failures of design, not attributes of information”. He highlightes that we are supposed to cut the clutter and get rid of everything that is not necessary to visualize the data.\nConsider the next two graphs. I made two bar graphs with a toy data frame and a binary outcome to keep it as simple as possible. I took my quite some time to create a graph that outlines the idea. As the plot on the left side shows, I created a theme that is supposed to look like the old Excel theme with a lot of clutter: The background is gray, I colored the bars even though the color and the legend do not transport any information, and I picked thick, black grid lines for a finishing touch. To compare this ugly beast, the right side shows the ggplot2 default version. Unfold the code if you want to create a ugly, cluttered graph on your own.\n\n\n\n\n\nThe reinvention of the old Excel theme seems a bit drastic, but even the default ggplot2 theme has some clutter that we could get rid of. This might not be necessary, but it highlights that there is always room to improve a graph, especially when it comes to clutter. For example, we could use a different theme to get rid of the gray background, there is no need to color each bar since they do not represent information, and we could integrate a label for each bar to communicate clearly.\nSo, fill the bars white and make the border of the bars black. In addition, use a theme without background colors and provide a descriptive title.\n\n# De-color de bars\nggplot(df_clutter, aes(x = outcome, y = count)) +\n  geom_col(color = \"black\", fill = \"white\") +\n  theme_minimal(base_size = 12) +\n  labs(title = \"Members by Sex\")\n\n\n\n\nNext, the geom_text() helps us to integrate text labels. Essentially, the function displays texts as a geometrical object which is why the main logic is not different compared to other geoms. I added a simple data frame (df_text) to illustrate how the geom works. It contains coordinates for x and y and an example text to visualize.\n\ndf_text &lt;- tibble::tribble(\n  ~x, ~y, ~text, ~group,\n  -1, -1, \"bottom left\", \"B\",\n  -1, 1, \"top right\", \"A\",\n  1, 1, \"top left\", \"A\",\n  1, -1, \"bottom right\", \"B\",\n  0, 0, \"center\", \"C\"\n)\n\nAs the next console highlights, the function depicts the text in accordance with the x and y coordinate, as the plot on the right side shows. The geom understands supplementary aesthetics and options (e.g., size, fontface) to display text. To give you an idea how it works, add the color aesthetics for each group and adjust the alignment of the text with the vjust (vertical adjustment) and the hjust (horizontal adjustment) option. If you set them to inward, the text will be aligned towards the center, but there are more alignment options available (e.g., left, right, center) should you prefer those.\n\n# geom_text example\np1 &lt;- ggplot(df_text, aes(x, y)) +\n  geom_text(aes(label = text),\n    size = 3\n  )\n\n# insert color aesthetic and adjust options (e.g., size, fontface)\np2 &lt;- ggplot(df_text, aes(x, y, color = group)) +\n  geom_text(aes(label = text),\n    vjust = \"inward\",\n    hjust = \"inward\",\n    size = 3,\n    fontface = \"bold\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\np1 + p2\n\n\n\n\nSince the data does not contain text to improve the bar graph, we may use the paste() function to create a label. It contains the group level, a new line (\\n), and the percentages.\n\n# Paste a label\npaste0(df_clutter$outcome, \"\\n\", df_clutter$percent, \"%\")\n\n#&gt; [1] \"Men\\n64.8%\"   \"Women\\n35.2%\"\n\n\nInclude the latter as a label and adjust the position via the y parameter Use the count and adjust it by increasing (decreasing) it manually. In addition, pick a text color and a reasonable text size.\n\n#Include text labels inside the bars\nggplot(df_clutter, aes(x=outcome, y=count)) +\n  geom_col(color = \"black\", fill = \"white\")+\n  geom_text(aes(label = paste0(outcome, \"\\n\", percent, \"%\"), \n                y = count - 100),\n            color=\"black\",\n            size = 3.5)+\n  theme_minimal(base_size = 12)+\n  labs(title= \"Members by Sex\")"
  },
  {
    "objectID": "chapter_07.html#summary",
    "href": "chapter_07.html#summary",
    "title": "6  Visualize data",
    "section": "6.5 Summary",
    "text": "6.5 Summary\nI highlighted several books to improve your ggplot2 and data visualizations skills, but at the end of the day your skills will improve faster, if you start to visualize data on your own and accept that trial and error are not necessarily a wrong approach. To this end, the ggplot2 cheat sheet (from the package website) will support you as well.\nIn addition, keep the following functions and packages from Chapter 7 in mind:\n\nCreate a new ggplot (ggplot), aesthetic mappings (aes), and add a geom_* (e.g., geom_bar, geom_point, geom_smooth)\nAdd a layer with +, start each new function on a new line, don’t forget to delete the plus sign if you delete the last line of code\nThere are several predefined theme functions (e.g., theme_bw, theme_light).\nModify axis, legend, and plot labels (e.g., with labs)\nLay out panels in a grid (e.g., facet_grid)\nDiscard (or adjust) the legend (e.g., theme(legend.position = \"none\"))\nAdjust the coordinate system (e.g., coord_cartesian)\nFurther packages:\n\nThemes: ggthemes (Arnold 2021)\nFont types: showtext (Qiu 2022)\nColor: The RColorBrewer (Neuwirth 2022) and the viridis package (Garnier 2021)\nMany color palettes: paletteer (Hvitfeldt 2021)\nCombine graphs: patchwork (Pedersen 2022b)\nZoom in: ggforce (Pedersen 2022a)\n\n\n\n\n\n\nArnold, Jeffrey B. 2021. ggthemes: Extra Themes, Scales and Geoms for ggplot2. https://CRAN.R-project.org/package=ggthemes.\n\n\nCairo, Alberto. 2016. The Truthful Art: Data, Charts, and Maps for Communication. New Riders.\n\n\nGarnier, Simon. 2021. viridis: Colorblind-Friendly Color Maps for R. https://CRAN.R-project.org/package=viridis.\n\n\nHvitfeldt, Emil. 2021. paletteer: Comprehensive Collection of Color Palettes. https://CRAN.R-project.org/package=paletteer.\n\n\nKunst, Joshua. 2022. highcharter: A Wrapper for the Highcharts Library. https://CRAN.R-project.org/package=highcharter.\n\n\nLüdecke, Daniel, Dominique Makowski, Indrajeet Patil, Mattan S. Ben-Shachar, Brenton M. Wiernik, and Philip Waggoner. 2022. see: Model Visualisation Toolbox for easystats and ggplot2. https://CRAN.R-project.org/package=see.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nPatil, Indrajeet. 2023. ggstatsplot: ggplot2 Based Plots with Statistical Details. https://CRAN.R-project.org/package=ggstatsplot.\n\n\nPedersen, Thomas Lin. 2022a. ggforce: Accelerating ggplot2. https://CRAN.R-project.org/package=ggforce.\n\n\n———. 2022b. patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nQiu, Yixuan. 2022. showtext: Using Fonts More Easily in R Graphs. https://CRAN.R-project.org/package=showtext.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2022. plotly: Create Interactive Web Graphics via plotly.js. https://CRAN.R-project.org/package=plotly.\n\n\nTreischl, Edgar J. 2023. Practice R: An Interactive Textbook. De Gruyter Oldenbourg.\n\n\nWickham, Hadley. 2021. babynames: US Baby Names 1880-2017. https://github.com/hadley/babynames.\n\n\n———. 2022. forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arnold, Jeffrey B. 2021. ggthemes: Extra\nThemes, Scales and Geoms for ggplot2. https://CRAN.R-project.org/package=ggthemes.\n\n\nBen-Shachar, Mattan S., Dominique Makowski, Daniel Lüdecke, Indrajeet\nPatil, and Brenton M. Wiernik. 2022. effectsize: Indices of Effect Size. https://CRAN.R-project.org/package=effectsize.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and\nLuke Sonnet. 2022. estimatr: Fast Estimators\nfor Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nCairo, Alberto. 2016. The Truthful Art:\nData, Charts, and Maps for\nCommunication. New Riders.\n\n\nComtois, Dominic. 2022. summarytools: Tools to\nQuickly and Neatly Summarize Data. https://CRAN.R-project.org/package=summarytools.\n\n\nCui, Boxuan. 2020. DataExplorer: Automate Data\nExploration and Treatment. https://CRAN.R-project.org/package=DataExplorer.\n\n\nFirke, Sam. 2021. janitor: Simple Tools for\nExamining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGarnier, Simon. 2021. viridis:\nColorblind-Friendly Color Maps for R. https://CRAN.R-project.org/package=viridis.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin\nData. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nHothorn, Torsten, Achim Zeileis, Richard W. Farebrother, and Clint\nCummins. 2022. lmtest: Testing Linear\nRegression Models. https://CRAN.R-project.org/package=lmtest.\n\n\nHugh-Jones, David. 2022. huxtable: Easily\nCreate and Style Tables for LaTeX, HTML and Other Formats.\nhttps://CRAN.R-project.org/package=huxtable/.\n\n\nHvitfeldt, Emil. 2021. paletteer: Comprehensive\nCollection of Color Palettes. https://CRAN.R-project.org/package=paletteer.\n\n\nKunst, Joshua. 2022. highcharter: A Wrapper for\nthe Highcharts Library. https://CRAN.R-project.org/package=highcharter.\n\n\nLong, Jacob A. 2021. interactions:\nComprehensive, User-Friendly Toolkit for Probing\nInteractions. https://CRAN.R-project.org/package=interactions.\n\n\n———. 2022. jtools: Analysis and Presentation of\nSocial Scientific Data. https://CRAN.R-project.org/package=jtools.\n\n\nLüdecke, Daniel, Dominique Makowski, Mattan S. Ben-Shachar, Indrajeet\nPatil, Philip Waggoner, and Brenton M. Wiernik. 2022. performance: Assessment of Regression Models\nPerformance. https://CRAN.R-project.org/package=performance.\n\n\nLüdecke, Daniel, Dominique Makowski, Indrajeet Patil, Mattan S.\nBen-Shachar, Brenton M. Wiernik, and Philip Waggoner. 2022. see: Model Visualisation Toolbox for easystats and\nggplot2. https://CRAN.R-project.org/package=see.\n\n\nMakowski, Dominique, Brenton M. Wiernik, Indrajeet Patil, Daniel\nLüdecke, and Mattan S. Ben-Shachar. 2022. Correlation:\nMethods for Correlation Analysis. https://CRAN.R-project.org/package=correlation.\n\n\nMüller, Kirill, and Hadley Wickham. 2022a. pillar: Coloured Formatting for Columns. https://CRAN.R-project.org/package=pillar.\n\n\n———. 2022b. tibble: Simple Data\nFrames. https://CRAN.R-project.org/package=tibble.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer\nPalettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nPatil, Indrajeet. 2023. ggstatsplot: ggplot2\nBased Plots with Statistical Details. https://CRAN.R-project.org/package=ggstatsplot.\n\n\nPedersen, Thomas Lin. 2022a. ggforce:\nAccelerating ggplot2. https://CRAN.R-project.org/package=ggforce.\n\n\n———. 2022b. patchwork: The Composer of\nPlots. https://CRAN.R-project.org/package=patchwork.\n\n\nQiu, Yixuan. 2022. showtext: Using Fonts More\nEasily in R Graphs. https://CRAN.R-project.org/package=showtext.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz\nMarbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2022. plotly: Create Interactive Web Graphics via\nplotly.js. https://CRAN.R-project.org/package=plotly.\n\n\nSolt, Frederick, and Yue Hu. 2021. dotwhisker:\nDot-and-Whisker Plots of Regression Results. https://CRAN.R-project.org/package=dotwhisker.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations\nfor Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTreischl, Edgar J. 2023. Practice R: An\nInteractive Textbook. De Gruyter Oldenbourg.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. skimr:\nCompact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. babynames: US Baby Names\n1880-2017. https://github.com/hadley/babynames.\n\n\n———. 2022. forcats: Tools for Working with\nCategorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, and Jennifer Bryan. 2022. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. ggplot2: Create Elegant Data\nVisualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\ndplyr: A Grammar of Data\nManipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Maximilian Girlich. 2022. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2022. haven: Import and Export SPSS, Stata and SAS\nFiles. https://CRAN.R-project.org/package=haven."
  }
]